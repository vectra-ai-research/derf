{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DeRF Documentation Home","text":"<p>The DeRF, an open-source tool available on GitHub, consists of Terraform modules and a Cloud Run application written in Python. Within this package, a variety of built-in attack techniques are provided, focusing on targeting AWS and GCP. For a complete accounting of all built-in attack techniques, refer to the list in documentation.    The DeRF deploys and manages the target cloud infrastructure, which is manipulated to simulate attacker techniques. Terraform is used to manage all resources, deploying (and destroying) hosted attack techniques and target infrastructure in under 3 minutes.    While a bring-your-own-Infrastructure (BYOI) model isn't currently supported, maintaining The DeRF infrastructure costs less than $10/month for Google Cloud and $5/month for AWS. The tool's convenient deployment model means you can use it as needed rather than continuously running 24/7. Check out the deployment guide for more details.</p>"},{"location":"#key-features-of-this-tool-include","title":"Key features of this tool include:","text":"<p>\u2022   User-Friendly Interface: Since the DeRF is hosted in Google Cloud, End Users can invoke attacks through the cloud console UI without the need to install software or use the CLI.</p> <p>\u2022   Accessibility for Non-Security Professionals: The DeRF caters to a broad audience of End Users, including Engineering, Sales, Support Staff, or automated processes.</p> <p>\u2022   Robust OpSec: Long-Lived Credentials are not passed between operators, instead access to the DeRF and its attack techniques are controlled through GCP IAM Role-Based Access Control (RBAC)</p> <p>\u2022   Extensibility at its Core: Attack sequences are written in YAML, enabling easy configuration of new techniques.</p> <p>\u2022   Turn-Key deployment: Deploying (and destroying!) the DeRF is a fully automated process, completed in under 3 minutes.</p>"},{"location":"#high-level-architecture","title":"High Level Architecture","text":"<p>The DeRF\u2019s unique architecture is wholly deployed via terraform.  It consists of resources spread across AWS and GCP.</p>"},{"location":"#derf-attack-architecture-for-aws","title":"DeRF Attack Architecture for AWS","text":""},{"location":"#derf-attack-architecture-for-gcp","title":"DeRF Attack Architecture for GCP","text":""},{"location":"#derf-user-personas","title":"DeRF User Personas","text":"<p>See the User Guide for more detailed descriptions of the permissions assigned to the DeRF Execution and Default Users.</p> <p> </p> <p>The DeRF Deployment User deploys the DeRF terraform module across AWS and GCP. Permissions required for the DeRF Deployment User are documented here </p> <p></p> <p> </p> <p>The DeRF Execution User 01 is one of two built in an AWS IAM Users which AS attack techniques can run as. Permissions are assigned to the <code>derf-execution-users</code> AWS IAM Group and documented within each attack module. </p> <p> </p> <p>The DeRF Execution User 02 is one of two built in an AWS IAM Users which AWS attack techniques can run as. Permissions are assigned to the <code>derf-execution-users</code> AWS IAM Group and documented within each attack module. </p> <p> </p> <p>The DeRF Default User is an AWS IAM User used by attack techniques to revert state changing actions by the AWS attack modules.  If AWS attack techniques are run with the user parameter left blank, the attack with default to run as this user.  </p> <p> </p> <p>In order to perform an attack as an arbitrary AWS Role, AWS IAM Temporary Session Credentials generated from IAM Roles can be passed directly to the <code>aws-proxy-app</code> as Post Body Parameters additionally with the <code>TEMPCREDSPASSED = yes</code> Post Body parameter. </p> <p> </p> <p>The DeRF Service Account 01 is one of two built in Service Accounts which GCP attack techniques can run as. Roles are assigned to these two DeRF Execution Service accounts in the targeted Project as part of tool deployment.   </p> <p> </p> <p> </p> <p>The DeRF Service Account 02 is one of two built in Service Accounts which GCP attack techniques can run as.Roles are assigned to these two DeRF Execution Service accounts in the targeted Project as part of tool deployment.   </p> <p> </p>"},{"location":"#video-demos","title":"Video Demo's","text":"<p>Watch this tutorial to understand, what is the DeRF, how to use it and how deploy in your environment. </p>"},{"location":"comparison/","title":"Comparison With Other Tools","text":"<p>This page is intended to help inform the end user as to which tool might be best for their use case, how the DeRF compares to existing tooling based on things like: -  Is the tool multi-cloud?  - Where are the attack techniques executed?  - How extensible is the tool?  - Are the attack techniques executed with a GUI, API or both?  - Does the tool manage the target infrastructure or does it use a bring-your-own-infrastructure (BYOI) model?   </p>"},{"location":"comparison/#stratus-red-team-by-data-dog","title":"Stratus Red Team by Data Dog","text":"<p>Stratus Red Team fashions itself as \"Atomic Red Team\u2122\", but focused on cloud.</p> <p>Stratus Red Team is a self-contained GO binary that can be used to detonate offensive attack techniques against a live cloud environments (AWS, GCP and Azure). It consists of a CLI tool which operators can 'detonate' individual attack techniques against an AWS, GCP or Azure target.  Each attack technique is a self-contained module which creates the infrastructure required for the attack in a warm-up phase, following which the attack is performed.  Finally, any created infrastructure is destroyed. While Stratus Red Team is an awesome tool for an individual operator, its not great for those less technical or when you need to democratize attack execution, making it invocation available to larger teams. </p> <p>DeRF has made the decision to release a predefined set of AWS and GCP attack techniques that aligns with the capabilities of Stratus Red Team. This choice ensures consistency among the publicly available tools. However, it's important to note that DeRF is designed to be extensible. Users are encouraged to develop their own attack techniques, allowing for customization and expansion based on their specific requirements.</p> <p>Use Stratus Red Team when: There is an individual, technical operator who needs to execute a set of pre-defined attack techniques in AWS, Azure, K8s or GCP.</p> <p>Use DeRF when:  1. There are a group of individuals who needs to execute attack techniques in AWS or GCP only. Especially consider the use of DeRF when the End User is less technical or attacks need to be automated and automation can easily authenticate against Google Cloud.   2. Or when you need to extend a tool, creating your own attack sequences.  3. Its also strong indication you might need to use The DeRF when the attack executor is different that the one deploying the tool or creating attack techniques.</p>"},{"location":"comparison/#atomic-red-team-by-red-canary","title":"Atomic Red Team by Red Canary","text":"<p>Credit: Description by Status Red Team</p> <p>Atomic Red Team\u2122 is library of tests mapped to the MITRE ATT&amp;CK\u00ae framework. Security teams can use Atomic Red Team to quickly, portably, and reproducibly test their environments.</p> <p>In 2021, Atomic Red Team added support for Cloud TTPs. In the summer 2022, Atomic Red Team also started leveraging Stratus Red Team to execute some of its cloud attack techniques.</p> <p>Atomic Red Team has very few cloud TTPs it implements itself. While Atomic Red Team is an awesome tool for endpoint security, it wasn't built purposely for cloud environments. In particular, it doesn't handle the prerequisite infrastructure and configuration necessary to detonate TTPs, and leaves that to the user.  For instance, AWS - Create Access Key and Secret Key requires you to create an IAM user prior to detonating the attack. Both the DeRF and Stratus Red Team packages this prerequisite logic, so you can detonate attack techniques without having to create any infrastructure or cloud configuration manually.</p>"},{"location":"comparison/#atomic-red-team-versus-the-derf","title":"Atomic Red Team versus the DeRF","text":"<ul> <li>Similarities: Attack techniques in the DeRF and Atomic Red Team are both based on YAML</li> <li>Infrastructure Differences: Unlike Atomic Red Team, The DeRF fully manages the target infrastructure while Atomic Red Team operates on a bring-your-own-infrastructure (BYOI) model.</li> <li>Cloud Coverage Differences: Atomic Red Team focuses on executing TTPs mapped to MITRE primarily targeting on-premises infrastructure while the DeRF is wholely cloud focused.</li> <li>Usage Differences: Atomic Red Team implements TTPs which can be programatically executed, while the DeRF has built-in attack techniques which are executed either with a GUI or via an API.</li> </ul>"},{"location":"comparison/#leonidas-by-withsecure-nick-jones","title":"Leonidas by WithSecure (Nick Jones)","text":"<p>Credit: Description by Status Red Team</p> <p>Leonidas is a framework for executing attacker actions in the cloud. It provides a YAML-based format for defining cloud attacker tactics, techniques and procedures (TTPs) and their associated detection properties</p> <p>While The DeRF, Stratus Red Team and Leonidas all have similar goals, their implementations are considerably different.</p>"},{"location":"comparison/#leonidas","title":"Leonidas","text":"<ul> <li>Leonidas is a fully-fledged web application you deploy in your AWS account using Terraform, and then a CodePipeline pipeline.</li> <li>Then, you use \"Leo\", the test case orchestrator, to hit the web API and detonate attack techniques. </li> <li>Leonidas allows describing TTPs as YAML, making it easier to extend than Stratus Red Team. </li> <li>Leonidas does not handle prerequisites for detonating attack techniques.</li> <li>The attack techniques implemented by Leonidas are very granular, meaning it can be challenging to implement detection for them. See for instance: STS Get Caller Identity</li> <li>Leonidas comes with a set of suggested threat detection rules. However, as its attack techniques are very granular, it is practically impossible to use them as-is in a real production environment, as they would trigger many false positives.</li> </ul>"},{"location":"comparison/#leonidas-versus-stratus-red-team","title":"Leonidas versus Stratus Red Team","text":"<p>Stratus Red Team aims at being simpler to use (single binary) and does not require you to have prior infrastructure or configuration in your AWS account. Stratus Red Team focuses on a single thing: executing cloud attack tactics against a live environment, with minimal overhead. You can also use Stratus Red Team programmatically, from Go code, as a library.</p>"},{"location":"comparison/#leonidas-versus-the-derf","title":"Leonidas versus the DeRF","text":"<ul> <li>Similarities: Similar to Leonidas, the attack framework for the DeRF is hosted in the cloud and the deployment of the tool versus the execution of the attacks can be performed by different users.  </li> <li>Infrastructure Differences: Unlike Leonidas, The DeRF fully manages the infrastructure which is targeted while Leonidas operates on a bring-your-own-infrastructure (BYOI) model. </li> <li>Cloud Coverage Differences: Leonidas only implements test cases for AWS while the DeRF has built in attack techniques for both AWS, GCP and is extensible to any http target.</li> <li>Usage Differences: Leonidas implements test cases which can be programatically executed only while the DeRF has built-in attack techniques which are executed either with a GUI or via an API.</li> </ul>"},{"location":"comparison/#pacu-by-rhino-security-labs","title":"Pacu by Rhino Security Labs","text":"<p>Credit: Description by Status Red Team </p> <p>Pacu is an open-source AWS exploitation framework, designed for offensive security testing against cloud environments. Created and maintained by Rhino Security Labs, Pacu allows penetration testers to exploit configuration flaws within an AWS account, using modules to easily expand its functionality.</p> <p>Pacu is an offensive AWS exploitation framework, aimed at penetration testers. It implements various enumeration and exploitation methods, some straightforward and some advanced. For instance, lambda__backdoor_new_roles creates a Lambda function and a CloudWatch Event causing all future IAM roles created in an AWS account to be backdoored automatically. Pacu aims at being used against existing AWS infrastructure. </p>"},{"location":"comparison/#pacu-versus-the-derf","title":"Pacu versus the DeRF","text":"<ul> <li>Similarities: Both tools execute attack techniques in the cloud.</li> <li>Infrastructure Differences: Unlike Pacu, The DeRF fully manages the infrastructure which is targeted while Pacu operates on a bring-your-own-infrastructure (BYOI) model making it a better choice for red teamers and pentesters.</li> <li>Cloud Coverage Differences: Pacu only implements attack modules for AWS while the DeRF has built in attack techniques for both AWS, GCP and is extensible to any http target.</li> <li>Usage Differences: Pacu implements modules which are programatically executed,   while the DeRF has built-in attack techniques which are executed either with a GUI or via an API.</li> </ul>"},{"location":"comparison/#amazon-guardduty-tester-by-aws","title":"Amazon GuardDuty Tester by AWS","text":"<p>Credit: Description by Status Red Team</p> <p>Amazon GuardDuty Tester is helpful to trigger GuardDuty findings. However, it is tightly coupled with GuardDuty and is a product-specific tool, even within the AWS ecosystem. If GuardDuty doesn't detect an attack technique, you won't find it in here.</p>"},{"location":"comparison/#aws-cloudsaga-by-aws","title":"AWS CloudSaga by AWS","text":""},{"location":"comparison/#credit-description-by-status-red-team","title":"Credit: Description by Status Red Team","text":"<p>AWS CloudSaga has a few simulation scenarios that cover both audit and attack goals. Some of them are more focused around identifying vulnerable resources in your account (audit focused) (such as <code>imds_reveal</code> listing your EC2 instances without IMDSv2 enforced), while others are designed to simulate attacker behavior.</p>"},{"location":"comparison/#aws-cloudsaga-versus-the-derf","title":"AWS CloudSaga versus the DeRF","text":"<ul> <li>Similarities: Both tools describe and execute attack techniques in the cloud.</li> <li>Philosophy Differences: The attacker behavior implemented by AWS Cloud Saga emulates several stages of the attack lifecycle, while the built-in attack techniques in the DeRF are more granular, representing attacker behaviors rather than larger attack lifecycles.</li> <li>Cloud Coverage Differences: CloudSaga only implements attack modules for AWS while the DeRF has built in attack techniques for both AWS, GCP and is extensible to any http target.</li> <li>Usage Differences: CloudSaga implements modules which are programatically executed,  only while the DeRF has built-in attack techniques targeting both AWS and GCP which are executed either with a GUI or via an API.</li> </ul>"},{"location":"comparison/#cloudgoat-by-rhino-security-labs","title":"CloudGoat by Rhino Security Labs","text":"<p>Credit: Description by Status Red Team</p> <p>CloudGoat is Rhino Security Labs' \"Vulnerable by Design\" AWS deployment tool. It allows you to hone your cloud cybersecurity skills by creating and completing several \"capture-the-flag\" style scenarios.</p> <p>CloudGoat is focused on spinning up vulnerable AWS infrastructure, so that you can exploit it to find a flag through a complete exploitation chain.</p> <p>Use CloudGoat to: practice your AWS offensive security and enumeration skills.</p> <p>Use tools like the DeRF or Stratus Red Team to: emulate adversary behavior, validate your detection logic or perform controls validation.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>We welcome pull requests, contributions and feedback! For any bug report or feedback, open an issue.</p>"},{"location":"contributing/#contributing-to-a-new-attack-technique","title":"Contributing to a new attack technique","text":"<p>When contributing a new public attack technique (modules under the <code>attack-technique</code> directory), please submit a pull request. Locate a sample module in the <code>attack-internal</code> directory for an example of module structure.</p>"},{"location":"contributing/#contributing-to-the-core-of-derf","title":"Contributing to the core of DeRF","text":"<p>When contributing to the core of DeRF (any code under the <code>/aws-proxy-app</code> or <code>derf-deployment</code> directory), please submit a pull request.</p>"},{"location":"faq/","title":"F.A.Q.","text":""},{"location":"faq/#what-permissions-do-i-need-to-run-derf","title":"What permissions do I need to run DeRF?","text":"<p>DeRF separates the permissions required to deploy the infrastructure from the permissions required to execute an attack.</p>"},{"location":"faq/#deployment-permissions","title":"Deployment Permissions","text":"<p>See here for AWS policy detailing the permissions required to in AWS and GCP to deploy the DeRF.</p>"},{"location":"faq/#end-user-execution-permissions","title":"End User Execution Permissions","text":"<p>See here for detailed instructions on the permissions required in GCP to execute attacks.</p>"},{"location":"faq/#how-does-the-derf-persist-state","title":"How does the DeRF persist state?","text":"<p>The DeRF uses a remote backend for its Terraform configurations and as such, a S3 bucket is required to initialize and deploy The DeRF.  See <code>./env-prod/TEMPLATE.conf</code>.   This is a requirement because AWS Access Keys are generated during DeRF deployment and the Access Key Secret value will persist in terraform state.  Its better these secrets persists in an encrypted S3 bucket with appropriate access controls rather than the operators local machine. </p>"},{"location":"faq/#how-can-i-add-my-own-attack-techniques-to-the-derf","title":"How can I add my own attack techniques to the DeRF?","text":"<p>Review the documentation at <code>docs/user-guide/aws-attack-creation.md</code> for instructions on creating your own attacks targeting AWS resources.  See sample attack modules under the <code>attacks-internal</code> directory.</p>"},{"location":"faq/#can-i-use-the-derf-to-execute-attack-techniques-against-my-own-infrastructure","title":"Can I use the DeRF to execute attack techniques against my own infrastructure?","text":"<ul> <li>AWS: This tool spins up DeRF specific resources in specified AWS account in order for the attack techniques to operate on.  Within each module you can see the required resources in the <code>infra.tf</code> file.  If you wanted the attack technique modules to target different infrastructure it would require some terraform surgery.  <ol> <li>Comment out the contents of the <code>infra.tf</code> file.</li> <li>Within the <code>local.tf</code> file, replace the values of the local variables with the hardcoded values from your BYO Infrastructure.</li> <li>Comment out the variables from the infrastructure no longer used in the <code>variables.tf</code> file.</li> </ol> </li> </ul>"},{"location":"faq/#how-do-i-destroy-the-derf-infrastructure","title":"How do I destroy the DeRF Infrastructure?","text":"<p>All deployed resources across both AWS and GCP will be removed with terraform. From the <code>env-prod/</code> directory: <pre><code>terraform destroy -var-file=derf.tfvars\n</code></pre></p>"},{"location":"faq/#is-this-a-good-pentesting-tool","title":"Is this a good pentesting tool?","text":"<p>Not really. The DeRF targets resources which are created and managed by the tool, it doesn't make a good tool for targeting arbitrary, un-managed infrastructure. If you are looking for a good pentesting tool for AWS, checkout pacu, for Azure, checkout Microburst or the MAAD Framework</p>"},{"location":"Deployment/deployment-permissions/","title":"Deployment Permissions","text":""},{"location":"Deployment/deployment-permissions/#deployment-permissions","title":"Deployment Permissions","text":""},{"location":"Deployment/deployment-permissions/#aws","title":"AWS","text":"<p>After attempting to document individually the AWS IAM permissions required to deploy and destroy The DeRF via terraform, I gave up when the list of permissions became extensive and were \"effective admin\".  I am recommending The DeRF be deployed to an targeted AWS Account with a user having the AdministratorAccess Managed Policy.</p>"},{"location":"Deployment/deployment-permissions/#gcp-deployment","title":"GCP Deployment","text":"<p>Below are the Google Managed Roles required to deploy the DeRF into a Google Project.  It does not take into account the permissions required to create the project in the first place.</p> <ul> <li>roles/secretmanager.admin applied at the Project-Level<ul> <li>Required to create Secrets used to store AWS Access Key Id and Secrets and assign Roles at the Secret-Level.</li> </ul> </li> <li>roles/run.admin applied at the Project-Level<ul> <li>Required to deploy the Cloud Run <code>aws-proxy-app</code></li> </ul> </li> <li>roles/artifactregistry.admin applied at the Project-Level<ul> <li>Required when triggering Cloud Builds to store built image artifacts</li> </ul> </li> <li>roles/workflows.admin applied at the Project-Level<ul> <li>Required to deploy and invoke Google Workflows</li> </ul> </li> <li>roles/resourcemanager.projectIamAdmin applied at the Project-Level<ul> <li>Required to set IAM Policy at the Project level.</li> </ul> </li> </ul>"},{"location":"Deployment/derf-deployment/","title":"DeRF Deployment","text":""},{"location":"Deployment/derf-deployment/#deployment-steps","title":"Deployment Steps","text":"<ol> <li>Complete Prerequisites see below.</li> <li>Complete System Requirements see below.</li> <li>Clone the Github repo to your local system. <pre><code>git clone https://github.com/vectra-ai-research/derf.git\n</code></pre></li> <li>Deploy the DeRF via Terraform from the <code>./env-prod</code> directory.  <pre><code>export AWS_PROFILE=PROFILE   \n</code></pre></li> </ol> <pre><code>terraform init -backend-config=derf.conf   \n</code></pre> <pre><code>terraform plan -var-file=derf.tfvars   \n</code></pre> <pre><code>terraform apply -var-file=derf.tfvars   \n</code></pre>"},{"location":"Deployment/derf-deployment/#prerequisites","title":"Prerequisites","text":"<ol> <li>Cloud Accounts:<ul> <li>One AWS Account: <ul> <li>This is your targeted AWS Account where attacks will run.</li> </ul> </li> <li>TWO GCP Projects: <ul> <li>Deployment Project: A Google Cloud Project which will house the DeRF <code>aws-proxy-app</code> </li> <li>Target Project: A Google Cloud Project which will be the target of attack techniques. </li> </ul> </li> </ul> </li> <li>Terraform Variables<ul> <li>Fill out the values the <code>TEMPLATE.tfvars</code> file located in <code>./env-prod</code> directory.</li> <li>Rename this file to be <code>derf.tfvars</code></li> <li>Terraform Variables:<ul> <li>aws_account_id: The Account ID to deploy the target AWS resources in.</li> <li>gcp_deployment_project_id: The ID of the GCP Project to deploy the DeRF Framework components.  This value may consist of both letters and numbers but never numbers alone.</li> <li>gcp_derf_project_id: The ID of the GCP Project to deploy target resources. This value may consist of both letters and numbers but never numbers alone.</li> </ul> </li> </ul> </li> <li>Backend Configuration<ul> <li>Fill out the values the <code>TEMPLATE.conf</code> file located in <code>./env-prod</code> directory.</li> <li>Rename this file to be <code>derf.conf</code></li> <li>Why run Terraform with a remote backend?</li> <li>Running a remote backend to an encrypted S3 bucket is recommended as AWS Access Keys are generated during this <code>Terraform Apply</code> and will otherwise be retained locally in the state file.</li> <li>Backend configuration values:<ul> <li>bucket: Name of the S3 bucket to store remote Terraform State. This should minimally be SSE-S3 encrypted. <ul> <li>Example: \"my-bucket-002984\"</li> </ul> </li> <li>region: The region your backend bucket is located in.</li> </ul> </li> </ul> </li> </ol>"},{"location":"Deployment/derf-deployment/#system-requirements","title":"System Requirements","text":"Terraform Installation <p>The DeRF has been tested verified with Terraform version <code>1.6.0</code>.  In order to manage multiple versions of Terraform on your system, install <code>tfenv</code> command line tool, allowing you to switch between different versions of terraform. </p> <ol> <li>Install <code>tfenv</code> <pre><code>brew install tfenv\n</code></pre></li> <li>Install Terraform version 1.6.0 <pre><code>tfenv install 1.6.0\n</code></pre></li> <li>Set version 1.6.0 as your default version <pre><code>tfenv use 1.6.0\n</code></pre></li> </ol> gcloud Installation <p>During deployment, the <code>gcloud</code> cli is invoked to trigger Cloud Build and deploy the <code>aws-proxy-app</code> to Cloud Run. As a result, <code>gcloud</code>  will need to be installed on your local system in order to deploy The DeRF.  </p> <p>Download and install the <code>gcloud</code> cli per instructions located here.</p>"},{"location":"Deployment/derf-deployment/#troubleshooting","title":"Troubleshooting","text":"<p>If you already have Terraform on your system, you may need to unlink the cask with the following command before <code>tfenv</code> will take over Terraform installation. <pre><code>brew unlink terraform\n</code></pre></p>"},{"location":"attack-techniques/list/","title":"Supported Platforms","text":"<p>Currently, The DeRF only comes with attack techniques for AWS and GCP.  See Getting Started for deployment instructions.</p>"},{"location":"attack-techniques/list/#list-of-all-attack-techniques","title":"List of all Attack Techniques","text":"<p>This page contains the list of all DeRF Attack Techniques.</p> Name Platform MITRE ATT&amp;CK Tactics Delete CloudTrail Trail AWS Defense Evasion Stop CloudTrail Logging AWS Defense Evasion Disable CloudTrail Logging Through Event Selectors AWS Defense Evasion CloudTrail Logs Impairment Through S3 Lifecycle Rule AWS Defense Evasion Attempt to Leave the aws Organization AWS Defense Evasion Remove VPC Flow Logs AWS Defense Evasion Exfiltrate EBS Snapshot by Sharing It AWS Exfiltration Exfiltrate an AMI by Sharing It AWS Exfiltration Exfiltrate RDS Snapshot by Sharing It AWS Exfiltration Open Ingress Port 22 on a Security Group AWS Exfiltration Download EC2 Instance User Data AWS Discovery Execute Discovery Commands on an EC2 Instance AWS Discovery Retrieve EC2 Password Data AWS Credential Access Steal EC2 Instance Credentials AWS Credential Access Retrieve and Decrypt SSM Parameters AWS Credential Access Retrieve a High Number of Secrets Manager secrets AWS Credential Access Execute Commands on EC2 Instance via User Data AWS Execution Launch Unusual EC2 Instances AWS Execution Console Login without MFA AWS Initial Access Impersonate GCP Service Accounts GCP Privilege Escalation Exfiltrate Compute Disk by sharing it GCP Exfiltration Backdoor a GCP Service Account through its IAM Policy GCP Persistence Exfiltrate Data from BigQuery Table via Unauthorized Query GCP Persistence"},{"location":"attack-techniques/aws/","title":"AWS","text":"<p>This page contains The DeRF attack techniques for available for AWS.</p>"},{"location":"attack-techniques/aws/#credential-access","title":"Credential Access","text":"<ul> <li> <p>Retrieve EC2 Password Data</p> </li> <li> <p>Steal EC2 Instance Credentials</p> </li> <li> <p>Retrieve a High Number of Secrets Manager secrets</p> </li> <li> <p>Retrieve And Decrypt SSM Parameters</p> </li> </ul>"},{"location":"attack-techniques/aws/#defense-evasion","title":"Defense Evasion","text":"<ul> <li> <p>Delete CloudTrail Trail</p> </li> <li> <p>Disable CloudTrail Logging Through Event Selectors</p> </li> <li> <p>CloudTrail Logs Impairment Through S3 Lifecycle Rule</p> </li> <li> <p>Stop CloudTrail Trail</p> </li> <li> <p>Attempt to Leave the AWS Organization</p> </li> <li> <p>Remove VPC Flow Logs</p> </li> </ul>"},{"location":"attack-techniques/aws/#discovery","title":"Discovery","text":"<ul> <li> <p>Execute Discovery Commands on an EC2 Instance</p> </li> <li> <p>Launch Unusual EC2 Instances</p> </li> </ul>"},{"location":"attack-techniques/aws/#execution","title":"Execution","text":"<ul> <li> <p>Execute Commands on EC2 Instance via User Data</p> </li> <li> <p>Launch Unusual EC2 Instances</p> </li> </ul>"},{"location":"attack-techniques/aws/#exfiltration","title":"Exfiltration","text":"<ul> <li> <p>Exfiltrate EBS Snapshot by Sharing It</p> </li> <li> <p>Exfiltrate an AMI by Sharing It</p> </li> <li> <p>Exfiltrate RDS Snapshot by Sharing It</p> </li> <li> <p>Open Ingress Port 22 on a Security Group</p> </li> </ul>"},{"location":"attack-techniques/aws/#privilege-escalation","title":"Privilege Escalation","text":"<ul> <li>Execute Commands on EC2 Instance via User Data</li> </ul>"},{"location":"attack-techniques/aws/#initial-access","title":"Initial Access","text":"<ul> <li>Console Login without MFA</li> </ul>"},{"location":"attack-techniques/aws/aws-console-login-without-mfa/","title":"Console Login without MFA","text":"<p>Platform: AWS</p>"},{"location":"attack-techniques/aws/aws-console-login-without-mfa/#mitre-attck-tactics","title":"MITRE ATT&amp;CK Tactics","text":"<ul> <li>Initial Access</li> </ul>"},{"location":"attack-techniques/aws/aws-console-login-without-mfa/#description","title":"Description","text":"<p>Simulates a login to the AWS Console for an IAM user without multi-factor authentication (MFA).</p>"},{"location":"attack-techniques/aws/aws-console-login-without-mfa/#attacker-actions","title":"Attacker Actions:","text":"<ul> <li>Logs into the AWS Console with a User that does not have MFA enabled.</li> <li>Resulting event name: <code>ConsoleLogin</code></li> <li>Assigned IAM Permission: NOne</li> </ul>"},{"location":"attack-techniques/aws/aws-console-login-without-mfa/#workflow-inputs","title":"Workflow Inputs:","text":"<p>None</p>"},{"location":"attack-techniques/aws/aws-console-login-without-mfa/#clean-up","title":"Clean Up:","text":"<p>None - no infrastructure modified</p>"},{"location":"attack-techniques/aws/aws-console-login-without-mfa/#execution-instructions","title":"Execution Instructions","text":"<ul> <li>See User Guide for Execution Instructions via the Google Cloud Console</li> <li>Programmatically execute this workflow with the following cli command:</li> </ul> <pre><code>gcloud workflows run aws-delete-cloudtrail-trail `--data={\"user\": \"user01\"}` \n</code></pre>"},{"location":"attack-techniques/aws/aws-console-login-without-mfa/#detection-artifacts","title":"Detection Artifacts","text":"<p>Using AWS <code>ConsoleLogin</code> event, the field <code>additionalEventData.MFAUser</code> is set to No when the IAM User did not use MFA to log into the console.</p> <p>Refer to Stratus Red Team documentation for additional detailed detection artifacts produced by this attack technique.</p>"},{"location":"attack-techniques/aws/cloudtrail-delete/","title":"Delete CloudTrail Trail","text":"<p>Platform: AWS</p>"},{"location":"attack-techniques/aws/cloudtrail-delete/#mitre-attck-tactics","title":"MITRE ATT&amp;CK Tactics","text":"<ul> <li>Defense Evasion</li> </ul>"},{"location":"attack-techniques/aws/cloudtrail-delete/#description","title":"Description","text":"<p>Delete a CloudTrail trail simulating an attacker disrupting logging to evade detection.</p>"},{"location":"attack-techniques/aws/cloudtrail-delete/#attacker-actions","title":"Attacker Actions:","text":"<ul> <li>Deletes a CloudTrail trail.</li> <li>Resulting event name: <code>DeleteTrail</code></li> <li>Assigned IAM Permission: <code>cloudtrail:DeleteTrail</code></li> </ul>"},{"location":"attack-techniques/aws/cloudtrail-delete/#workflow-inputs","title":"Workflow Inputs:","text":"<p>Specify which user this attack should run as.  <pre><code>{\"user\":\"user01\"}\n{\"user\":\"user02\"}\n</code></pre></p>"},{"location":"attack-techniques/aws/cloudtrail-delete/#clean-up","title":"Clean Up:","text":"<ul> <li>Recreates the CloudTrail trail.</li> <li>Executed as the <code>DeRF Default User</code></li> </ul>"},{"location":"attack-techniques/aws/cloudtrail-delete/#execution-instructions","title":"Execution Instructions","text":"<ul> <li>See User Guide for Execution Instructions via the Google Cloud Console</li> <li>Programmatically execute this workflow with the following cli command:</li> </ul> <pre><code>gcloud workflows run aws-delete-cloudtrail-trail `--data={\"user\": \"user01\"}` \n</code></pre>"},{"location":"attack-techniques/aws/cloudtrail-delete/#detection-artifacts","title":"Detection Artifacts","text":"<p>Identify when a CloudTrail trail is deleted through the AWS event,  <code>DeleteTrail</code>.   </p> <p>Refer to Stratus Red Team documentation for additional detailed detection artifacts produced by this attack technique.</p>"},{"location":"attack-techniques/aws/cloudtrail-event-selectors/","title":"Disable CloudTrail Logging Through Event Selectors","text":"<p>Platform: AWS</p>"},{"location":"attack-techniques/aws/cloudtrail-event-selectors/#mitre-attck-tactics","title":"MITRE ATT&amp;CK Tactics","text":"<ul> <li>Defense Evasion</li> </ul>"},{"location":"attack-techniques/aws/cloudtrail-event-selectors/#description","title":"Description","text":"<p>Disrupt CloudTrail Logging by creating an event selector on the Trail, filtering out all management events.</p>"},{"location":"attack-techniques/aws/cloudtrail-event-selectors/#attacker-actions","title":"Attacker Actions:","text":"<ul> <li>Updates the in-scope events captured by a Cloudtrail to exclude all management-plane events.</li> <li>Resulting event name: <code>PutEventSelector</code></li> <li>Assigned IAM Permission: <code>cloudtrail:PutEventSelector</code></li> </ul>"},{"location":"attack-techniques/aws/cloudtrail-event-selectors/#workflow-inputs","title":"Workflow Inputs:","text":"<p>Specify which user this attack should run as.  <pre><code>{\"user\":\"user01\"}\n{\"user\":\"user02\"}\n</code></pre></p>"},{"location":"attack-techniques/aws/cloudtrail-event-selectors/#clean-up","title":"Clean Up:","text":"<ul> <li>Reverts the event selectors on Cloudtrail trail, resuming logging of management-plane events</li> <li>Executed as the <code>DeRF Default User</code></li> </ul>"},{"location":"attack-techniques/aws/cloudtrail-event-selectors/#execution-instructions","title":"Execution Instructions","text":"<ul> <li>See User Guide for Execution Instructions via the Google Cloud Console</li> <li>Programmatically execute this workflow with the following cli command:</li> </ul> <pre><code>gcloud workflows run aws-cloudtrail-event-selector-srt `--data={\"user\": \"user01\"}` \n</code></pre>"},{"location":"attack-techniques/aws/cloudtrail-event-selectors/#detection-artifacts","title":"Detection Artifacts","text":"<p>Identify when a CloudTrail trail is deleted through the AWS event,  <code>PutEventSelectors</code>.   </p> <p>Refer to Stratus Red Team documentation for additional detailed detection artifacts produced by this attack technique.</p>"},{"location":"attack-techniques/aws/cloudtrail-lifecycle-rules/","title":"CloudTrail Logs Impairment Through S3 Lifecycle Rule","text":"<p>Platform: AWS</p>"},{"location":"attack-techniques/aws/cloudtrail-lifecycle-rules/#mitre-attck-tactics","title":"MITRE ATT&amp;CK Tactics","text":"<ul> <li>Defense Evasion</li> </ul>"},{"location":"attack-techniques/aws/cloudtrail-lifecycle-rules/#description","title":"Description","text":"<p>Set a 1-day retention policy on the S3 bucket used by a CloudTrail Trail, using a S3 Lifecycle Rule.</p>"},{"location":"attack-techniques/aws/cloudtrail-lifecycle-rules/#attacker-actions","title":"Attacker Actions:","text":"<ul> <li>Updates the Lifecycle rule on the S3 bucket backing the CloudTrail trail to 1 day.</li> <li>Resulting event name: <code>PutBucketLifecycle</code></li> <li>Assigned IAM Permission: <code>s3:PutBucketLifecycle</code></li> </ul>"},{"location":"attack-techniques/aws/cloudtrail-lifecycle-rules/#workflow-inputs","title":"Workflow Inputs:","text":"<p>Specify which user this attack should run as.  <pre><code>{\"user\":\"user01\"}\n{\"user\":\"user02\"}\n</code></pre></p>"},{"location":"attack-techniques/aws/cloudtrail-lifecycle-rules/#clean-up","title":"Clean Up:","text":"<ul> <li>Resets Lifecycle configuration to 90 days.</li> <li>Executed as the <code>DeRF Default User</code></li> </ul>"},{"location":"attack-techniques/aws/cloudtrail-lifecycle-rules/#execution-instructions","title":"Execution Instructions","text":"<ul> <li>See User Guide for Execution Instructions via the Google Cloud Console</li> <li>Programmatically execute this workflow with the following cli command:</li> </ul> <pre><code>gcloud workflows run aws-cloudtrail-lifecycle-rule-srt `--data={\"user\": \"user01\"}` \n</code></pre>"},{"location":"attack-techniques/aws/cloudtrail-lifecycle-rules/#detection-artifacts","title":"Detection Artifacts","text":"<p>Identify when a CloudTrail trail record may be impaired, through S3's <code>PutBucketLifecycle</code> event, specifically when the requestParameters.LifecycleConfiguration.Rule indicated S3 objects will be deleted in 1 day.</p> <p></p> <p>This detection is used to identify when a lifecycle rule with a short expiration is applied to an S3 bucket used for CloudTrail logging.</p> <p>Refer to Stratus Red Team documentation for additional detailed detection artifacts produced by this attack technique.</p>"},{"location":"attack-techniques/aws/cloudtrail-stop/","title":"Stop CloudTrail Trail","text":"<p>Platform: AWS</p>"},{"location":"attack-techniques/aws/cloudtrail-stop/#mitre-attck-tactics","title":"MITRE ATT&amp;CK Tactics","text":"<ul> <li>Defense Evasion</li> </ul>"},{"location":"attack-techniques/aws/cloudtrail-stop/#description","title":"Description","text":"<p>Stop the recording of events from a CloudTrail trail simulating an attacker disrupting logging to evade detection.</p>"},{"location":"attack-techniques/aws/cloudtrail-stop/#attacker-actions","title":"Attacker Actions:","text":"<ul> <li>Stop a CloudTrail trail.</li> <li>Resulting event name: <code>StopTrail</code></li> <li>Assigned IAM Permission: <code>cloudtrail:StopTrail</code></li> </ul>"},{"location":"attack-techniques/aws/cloudtrail-stop/#workflow-inputs","title":"Workflow Inputs:","text":"<p>Specify which user this attack should run as.  <pre><code>{\"user\":\"user01\"}\n{\"user\":\"user02\"}\n</code></pre></p>"},{"location":"attack-techniques/aws/cloudtrail-stop/#clean-up","title":"Clean Up:","text":"<ul> <li>Restarts the CloudTrail trail.</li> <li>Executed as the <code>DeRF Default User</code></li> </ul>"},{"location":"attack-techniques/aws/cloudtrail-stop/#execution-instructions","title":"Execution Instructions","text":"<ul> <li>See User Guide for Execution Instructions via the Google Cloud Console</li> <li>Programmatically execute this workflow with the following cli command:</li> </ul> <pre><code>gcloud workflows run aws-delete-cloudtrail-trail `--data={\"user\": \"user01\"}` \n</code></pre>"},{"location":"attack-techniques/aws/cloudtrail-stop/#detection-artifacts","title":"Detection Artifacts","text":"<p>Identify when a CloudTrail trail is disabled through the AWS event,  <code>DeleteTrail</code>.   </p> <p>Refer to Stratus Red Team documentation for additional detailed detection artifacts produced by this attack technique.</p>"},{"location":"attack-techniques/aws/ec2-ami-share-snapshot/","title":"Exfiltrate an AMI by Sharing It","text":"<p>Platform: AWS</p>"},{"location":"attack-techniques/aws/ec2-ami-share-snapshot/#mitre-attck-tactics","title":"MITRE ATT&amp;CK Tactics","text":"<ul> <li>Exfiltration</li> </ul>"},{"location":"attack-techniques/aws/ec2-ami-share-snapshot/#description","title":"Description","text":"<p>This attack has two different ways to share an EBS Snapshot externally.  - The first option shares an AMI snapshot with {\"groups\":\"all\"} - everyone. - The second case shares an AMI Snapshot with an external AWS that is user defined. If none is defined the AMI snapshot is shared with account <code>012345678901</code></p>"},{"location":"attack-techniques/aws/ec2-ami-share-snapshot/#attacker-actions","title":"Attacker Actions:","text":"<ul> <li>Updated the attributes of an AMI image Snapshot to an external, fictitious AWS account or {\"groups\":\"all\"}</li> <li>Resulting event name: <code>ModifyImageAttribute</code></li> <li>Assigned IAM Permission: <code>ec2:ModifyImageAttribute</code></li> </ul>"},{"location":"attack-techniques/aws/ec2-ami-share-snapshot/#workflow-inputs","title":"Workflow Inputs:","text":"<p>Specify which user this attack should run as.  <pre><code># Sharing the AMI with group:all\n{\"case\":\"1\",\"user\":\"user01\"}\n{\"case\":\"1\",\"user\":\"user02\"}\n\n# Sharing the AMI with an external fictitious account. \n# Define the fictitious account ID as the externalAccountId, a 12 digit numeric string, example below.\n{\"case\":\"2\",\"user\":\"user01\",\"externalAccountId\":\"012345678901\"}\n{\"case\":\"2\",\"user\":\"user02\",\"externalAccountId\":\"012345678901\"}\n</code></pre></p>"},{"location":"attack-techniques/aws/ec2-ami-share-snapshot/#clean-up","title":"Clean Up:","text":"<ul> <li>Removes the fictitious external AWS account or {\"groups\":\"all\"} by calling the <code>ModifyImageAttribute</code> API again, this time removing the permission.</li> <li>Executed as the <code>DeRF Default User</code></li> </ul>"},{"location":"attack-techniques/aws/ec2-ami-share-snapshot/#execution-instructions","title":"Execution Instructions","text":"<ul> <li>See User Guide for Execution Instructions via the Google Cloud Console</li> <li>Programmatically execute this workflow with the following cli command:</li> </ul> <pre><code>gcloud workflows run aws-ec2-ami-share-snapshot-srt --data={\"case\":\"1\",\"user\":\"user01\"}  \n</code></pre>"},{"location":"attack-techniques/aws/ec2-ami-share-snapshot/#detection-artifacts","title":"Detection Artifacts","text":"<p>Identify when the permission to launch an AMI Image Snapshot is modified with the <code>ModifyImageAttribute</code> event.  Specifically when the requestParameters.attributeType is <code>launchPermission</code> indicating the permission for AMI are being modified and the <code>requestParameters.launchPermission.add.items</code> contains either an external AWS Account Id or {\"groups\":\"all\"}</p> <p></p> <p>Refer to Stratus Red Team documentation for additional detailed detection artifacts produced by this attack technique.</p>"},{"location":"attack-techniques/aws/ec2-execute-discovery-commands/","title":"Execute Discovery Commands on an EC2 Instance","text":"<p>Platform: AWS</p>"},{"location":"attack-techniques/aws/ec2-execute-discovery-commands/#mitre-attck-tactics","title":"MITRE ATT&amp;CK Tactics","text":"<ul> <li>Discovery</li> </ul>"},{"location":"attack-techniques/aws/ec2-execute-discovery-commands/#description","title":"Description","text":"<p>This simulates an attacker performing discovery actions from a compromised EC2 instance. The commands will be run under the identity of the EC2 instance role A smattering of discovery commands are run the targeted EC2 instance including:</p> <ul> <li>sts:GetCallerIdentity</li> <li>s3:ListBuckets</li> <li>iam:GetAccountSummary</li> <li>iam:ListRoles</li> <li>iam:ListUsers</li> <li>iam:GetAccountAuthorizationDetails</li> <li>ec2:DescribeSnapshots</li> <li>cloudtrail:DescribeTrails</li> <li>guardduty:ListDetectors</li> </ul>"},{"location":"attack-techniques/aws/ec2-execute-discovery-commands/#attacker-actions","title":"Attacker Actions:","text":"<p>The following AWS API calls are made from the targeted EC2 instance: - sts:GetCallerIdentity - s3:ListBuckets - iam:GetAccountSummary - iam:ListRoles - iam:ListUsers - iam:GetAccountAuthorizationDetails - ec2:DescribeSnapshots - cloudtrail:DescribeTrails - guardduty:ListDetectors</p>"},{"location":"attack-techniques/aws/ec2-execute-discovery-commands/#workflow-inputs","title":"Workflow Inputs:","text":"<p>Specify which user this attack should run as.  <pre><code>{\"user\":\"user01\"}\n{\"user\":\"user02\"}\n</code></pre></p>"},{"location":"attack-techniques/aws/ec2-execute-discovery-commands/#clean-up","title":"Clean Up:","text":"<p>None - no resources are modified.</p>"},{"location":"attack-techniques/aws/ec2-execute-discovery-commands/#execution-instructions","title":"Execution Instructions","text":"<ul> <li>See User Guide for Execution Instructions via the Google Cloud Console</li> <li>Programmatically execute this workflow with the following cli command:</li> </ul> <pre><code>gcloud workflows run aws-ec2-get-user-data `--data={\"user\": \"user01\"}` \n</code></pre>"},{"location":"attack-techniques/aws/ec2-execute-discovery-commands/#detection-artifacts","title":"Detection Artifacts","text":"<p>Refer to Stratus Red Team documentation for detailed detection artifacts produced by this attack technique.</p>"},{"location":"attack-techniques/aws/ec2-get-password-data/","title":"AWS Retrieve EC2 Password Data","text":"<p>Platform: AWS</p>"},{"location":"attack-techniques/aws/ec2-get-password-data/#mitre-attck-tactics","title":"MITRE ATT&amp;CK Tactics","text":"<ul> <li>Credential Access</li> </ul>"},{"location":"attack-techniques/aws/ec2-get-password-data/#description","title":"Description","text":"<p>Runs ec2:GetPasswordData (30) times over (30) different fictitious EC2 instances. This simulates an attacker attempting to retrieve RDP passwords on a high number of Windows EC2 instances.</p> <p>See https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_GetPasswordData.html</p>"},{"location":"attack-techniques/aws/ec2-get-password-data/#attacker-actions","title":"Attacker Actions:","text":"<p>Attempts to receive RDP password from fictitious EC2 Instance Id.    - Resulting event name: <code>GetPasswordData</code>    - Assigned IAM Permission: <code>ec2:GetPasswordData</code> </p>"},{"location":"attack-techniques/aws/ec2-get-password-data/#workflow-inputs","title":"Workflow Inputs:","text":"<p>Specify which user this attack should run as.  <pre><code>{\"user\":\"user01\"}\n{\"user\":\"user02\"}\n</code></pre></p>"},{"location":"attack-techniques/aws/ec2-get-password-data/#clean-up","title":"Clean Up:","text":"<p>None</p>"},{"location":"attack-techniques/aws/ec2-get-password-data/#execution-instructions","title":"Execution Instructions","text":"<ul> <li>See User Guide for Execution Instructions via the Google Cloud Console</li> <li>Programmatically execute this workflow with the following cli command:</li> </ul> <pre><code>gcloud workflows run aws-delete-cloudtrail-trail `--data={\"user\": \"user01\"}` \n</code></pre>"},{"location":"attack-techniques/aws/ec2-get-password-data/#detection-artifacts","title":"Detection Artifacts","text":"<p>Identify principals making a large number of ec2:GetPasswordData calls, using CloudTrail's GetPasswordData event.</p> <p>e</p>"},{"location":"attack-techniques/aws/ec2-get-user-data/","title":"EC2 Get User Data","text":"<p>Platform: AWS</p>"},{"location":"attack-techniques/aws/ec2-get-user-data/#mitre-attck-tactics","title":"MITRE ATT&amp;CK Tactics","text":"<ul> <li>Discovery</li> </ul>"},{"location":"attack-techniques/aws/ec2-get-user-data/#description","title":"Description","text":"<p>This simulates an attacker attempting to retrieve EC2 Instance User Data that frequently includes installation scripts and hard-coded secrets for deployment. This module results in an <code>Access Denied</code> error as the users are not granted the appropriate permissions</p>"},{"location":"attack-techniques/aws/ec2-get-user-data/#attacker-actions","title":"Attacker Actions:","text":"<ul> <li>Calls the <code>DescribeInstanceAttribute</code> API specifying the <code>userData</code> attribute (3) times on a fictitious EC2 Instance.</li> <li>Resulting event name: <code>DescribeInstanceAttribute</code></li> <li>Assigned IAM Permission: None</li> </ul>"},{"location":"attack-techniques/aws/ec2-get-user-data/#workflow-inputs","title":"Workflow Inputs:","text":"<p>Specify which user this attack should run as.  <pre><code>{\"user\":\"user01\"}\n{\"user\":\"user02\"}\n</code></pre></p>"},{"location":"attack-techniques/aws/ec2-get-user-data/#clean-up","title":"Clean Up:","text":"<p>None - no resources are modified.</p>"},{"location":"attack-techniques/aws/ec2-get-user-data/#execution-instructions","title":"Execution Instructions","text":"<ul> <li>See User Guide for Execution Instructions via the Google Cloud Console</li> <li>Programmatically execute this workflow with the following cli command:</li> </ul> <pre><code>gcloud workflows run aws-ec2-get-user-data `--data={\"user\": \"user01\"}` \n</code></pre>"},{"location":"attack-techniques/aws/ec2-get-user-data/#detection-artifacts","title":"Detection Artifacts","text":"<p>Identify when a CloudTrail trail is deleted, through CloudTrail's <code>DescribeInstanceAttribute</code> event.</p>"},{"location":"attack-techniques/aws/ec2-launch-unusual-instances/","title":"Launch Unusual EC2 Instances","text":"<p>Platform: AWS</p>"},{"location":"attack-techniques/aws/ec2-launch-unusual-instances/#mitre-attck-tactics","title":"MITRE ATT&amp;CK Tactics","text":"<ul> <li>Execution</li> </ul>"},{"location":"attack-techniques/aws/ec2-launch-unusual-instances/#description","title":"Description","text":"<p>Simulates an attacker attempting to spin up several high-powered EC2 instances (p2.xlarge) which are suitable to cryptomining.  This attack technique ultimately fails for a couple reasons.    1. First, the IAM role assigned to the Instance Role used in the attack doesn't have the ec2:RunInstances permission.      2. Secondly a service quota needs to be increased to allow for these p2.xlarge instances to be created.  This module submits the service request, requesting the increase but it takes several days for the request to process and may not be approved when the attack technique is executed.</p>"},{"location":"attack-techniques/aws/ec2-launch-unusual-instances/#attacker-actions","title":"Attacker Actions:","text":"<p>Attacker attempts to start (10) the EC2 instances.    - Resulting event name: <code>RunInstances</code>    - Required IAM Permission: <code>ec2:RunInstances</code></p>"},{"location":"attack-techniques/aws/ec2-launch-unusual-instances/#workflow-inputs","title":"Workflow Inputs:","text":"<p>Specify which user this attack should run as.  <pre><code>{\"user\":\"user01\"}\n{\"user\":\"user02\"}\n</code></pre></p>"},{"location":"attack-techniques/aws/ec2-launch-unusual-instances/#clean-up","title":"Clean Up:","text":"<p>None - no resources are modified.</p>"},{"location":"attack-techniques/aws/ec2-launch-unusual-instances/#execution-instructions","title":"Execution Instructions","text":"<ul> <li>See User Guide for Execution Instructions via the Google Cloud Console</li> <li>Programmatically execute this workflow with the following cli command:</li> </ul> <pre><code>gcloud workflows run aws-ec2-launch-unusual-instances-srt `--data={\"user\": \"user01\"}` \n</code></pre>"},{"location":"attack-techniques/aws/ec2-launch-unusual-instances/#detection-artifacts","title":"Detection Artifacts","text":"<p>The request parameters in the <code>RunInstances</code> AWS Event Name will reveal if unusual EC2 instances have been launched or have been attempted to be launched. </p> <p>Refer to Stratus Red Team documentation for additional detailed detection artifacts produced by this attack technique.</p>"},{"location":"attack-techniques/aws/ec2-modify-user-data/","title":"Execute Commands on EC2 Instance via User Data","text":"<p>Platform: AWS</p>"},{"location":"attack-techniques/aws/ec2-modify-user-data/#mitre-attck-tactics","title":"MITRE ATT&amp;CK Tactics","text":"<ul> <li>Execution</li> </ul>"},{"location":"attack-techniques/aws/ec2-modify-user-data/#description","title":"Description","text":"<p>Executes code on an EC2 instance as the unix root user through the modification of User Data.</p> <p>References:</p> <ul> <li>https://hackingthe.cloud/aws/exploitation/local-priv-esc-mod-instance-att/</li> <li>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html</li> </ul>"},{"location":"attack-techniques/aws/ec2-modify-user-data/#attacker-actions","title":"Attacker Actions:","text":"<ul> <li>Attacker stops the EC2 instance before modifying User Data.</li> <li>Resulting event name: <code>StopInstances</code></li> <li>Assigned IAM Permission: <code>ec2:StopInstances</code></li> <li>Attacker maliciously updates the User Data to be executed on the VM.</li> <li>Resulting event name: <code>ModifyInstanceAttribute</code></li> <li>Assigned IAM Permission: <code>ec2:ModifyInstanceAttribute</code></li> <li>Attacker restarts the EC2 instance in order for the code to execute on the machine.</li> <li>Resulting event name: <code>StartInstances</code></li> <li>Assigned IAM Permission: <code>ec2:StartInstances</code></li> </ul>"},{"location":"attack-techniques/aws/ec2-modify-user-data/#workflow-inputs","title":"Workflow Inputs:","text":"<p>Specify which user this attack should run as.  <pre><code>{\"user\":\"user01\"}\n{\"user\":\"user02\"}\n</code></pre></p>"},{"location":"attack-techniques/aws/ec2-modify-user-data/#clean-up","title":"Clean Up:","text":"<p>None - no resources are modified.</p>"},{"location":"attack-techniques/aws/ec2-modify-user-data/#execution-instructions","title":"Execution Instructions","text":"<ul> <li>See User Guide for Execution Instructions via the Google Cloud Console</li> <li>Programmatically execute this workflow with the following cli command:</li> </ul> <pre><code>gcloud workflows run aws-ec2-get-user-data `--data={\"user\": \"user01\"}` \n</code></pre>"},{"location":"attack-techniques/aws/ec2-modify-user-data/#detection-artifacts","title":"Detection Artifacts","text":"<p>Identify when the <code>ModifyInstanceAttribute</code> event occurs with requestParameters.userData non-empty and containing suspicious or unexpected data. It's generally not expected that the user data of an EC2 instance changes often, especially with the popularity of immutable machine images, provisioned before instantiation.</p>"},{"location":"attack-techniques/aws/ec2-security-group-open-port-22-ingress/","title":"Open Ingress Port 22 on a Security Group","text":"<p>Platform: AWS</p>"},{"location":"attack-techniques/aws/ec2-security-group-open-port-22-ingress/#mitre-attck-tactics","title":"MITRE ATT&amp;CK Tactics","text":"<ul> <li>Exfiltration</li> </ul>"},{"location":"attack-techniques/aws/ec2-security-group-open-port-22-ingress/#description","title":"Description","text":"<p>Simulates an attacker loosening network restrictions to allow incoming SSH connections to an EC2 instance.  This is done by creating an ingress rule in a Security Group on port 22 from the Internet (0.0.0.0/0).</p>"},{"location":"attack-techniques/aws/ec2-security-group-open-port-22-ingress/#attacker-actions","title":"Attacker Actions:","text":"<p>Attacker opens port 22 from the Internet (0.0.0.0/0) by updating inbound rules to a VPC security group:     - Resulting event name: <code>AuthorizeSecurityGroupIngress</code>    - Required IAM Permission: <code>ec2:AuthorizeSecurityGroupIngress</code></p>"},{"location":"attack-techniques/aws/ec2-security-group-open-port-22-ingress/#workflow-inputs","title":"Workflow Inputs:","text":"<p>Specify which user this attack should run as.  <pre><code>{\"user\":\"user01\"}\n{\"user\":\"user02\"}\n</code></pre></p>"},{"location":"attack-techniques/aws/ec2-security-group-open-port-22-ingress/#clean-up","title":"Clean Up:","text":"<p>DeRF Default Execution User reverts ingress rule update, closing the open port.</p>"},{"location":"attack-techniques/aws/ec2-security-group-open-port-22-ingress/#execution-instructions","title":"Execution Instructions","text":"<ul> <li>See User Guide for Execution Instructions via the Google Cloud Console</li> <li>Programmatically execute this workflow with the following cli command:</li> </ul> <pre><code>gcloud workflows run aws-ec2-launch-unusual-instances-srt `--data={\"user\": \"user01\"}` \n</code></pre>"},{"location":"attack-techniques/aws/ec2-security-group-open-port-22-ingress/#detection-artifacts","title":"Detection Artifacts","text":"<p>The request parameters in the <code>AuthorizeSecurityGroupIngress</code> AWS Event Name will reveal both the inbound allowed IP with the parameter <code>cidrIp</code> and the ports exposed, <code>fromPort</code> and <code>toPort</code>.</p> <p></p> <p>Refer to Stratus Red Team documentation for additional detailed detection artifacts produced by this attack technique.</p>"},{"location":"attack-techniques/aws/ec2-share-ebs-snapshot/","title":"EC2 Shared EBS Snapshot","text":"<p>Platform: AWS</p>"},{"location":"attack-techniques/aws/ec2-share-ebs-snapshot/#mitre-attck-tactics","title":"MITRE ATT&amp;CK Tactics","text":"<ul> <li>Exfiltration</li> </ul>"},{"location":"attack-techniques/aws/ec2-share-ebs-snapshot/#description","title":"Description","text":"<p>This attack shares an EBS Snapshot with an external, fictitious AWS account, (012345678912)</p>"},{"location":"attack-techniques/aws/ec2-share-ebs-snapshot/#attacker-actions","title":"Attacker Actions:","text":"<ul> <li>Updated the attributes of an EBS Snapshot to an external, fictitious AWS account.</li> <li>Resulting event name: <code>ModifySnapshotAttribute</code></li> <li>Assigned IAM Permission: <code>ec2:ModifySnapshotAttribute</code></li> </ul>"},{"location":"attack-techniques/aws/ec2-share-ebs-snapshot/#workflow-inputs","title":"Workflow Inputs:","text":"<p>Specify which user this attack should run as.  <pre><code>{\"user\":\"user01\"}\n{\"user\":\"user02\"}\n</code></pre></p>"},{"location":"attack-techniques/aws/ec2-share-ebs-snapshot/#clean-up","title":"Clean Up:","text":"<ul> <li>Removes the fictitious external AWS account by calling the <code>ModifySnapshotAttribute</code> API again, this time removing the permission.</li> <li>Executed as the <code>DeRF Default User</code></li> </ul>"},{"location":"attack-techniques/aws/ec2-share-ebs-snapshot/#execution-instructions","title":"Execution Instructions","text":"<ul> <li>See User Guide for Execution Instructions via the Google Cloud Console</li> <li>Programmatically execute this workflow with the following cli command:</li> </ul> <pre><code>gcloud workflows run aws-ec2-share-ebs-snapshot-srt --data={\"user\": \"user01\"}\n</code></pre>"},{"location":"attack-techniques/aws/ec2-share-ebs-snapshot/#detection-artifacts","title":"Detection Artifacts","text":"<p>Identify when an EBS Snapshot permission is modified with the  <code>ModifySnapshotAttribute</code> event, specifically when the requestParameters.createVolumePermission contains an \"add\" object\" and the key add.items[].userId is an external AWS Account.</p> <p></p> <p>Refer to Stratus Red Team documentation for additional detailed detection artifacts produced by this attack technique.</p>"},{"location":"attack-techniques/aws/ec2-steal-instance-credentials/","title":"AWS Steal EC2 Instance Credentials","text":"<p>Platform: AWS</p>"},{"location":"attack-techniques/aws/ec2-steal-instance-credentials/#mitre-attck-tactics","title":"MITRE ATT&amp;CK Tactics","text":"<ul> <li>Credential Access</li> </ul>"},{"location":"attack-techniques/aws/ec2-steal-instance-credentials/#description","title":"Description","text":"<p>Simulates the theft of EC2 instance credentials from the Instance Metadata Service and the use of the stolen credentials outside AWS IP space.</p>"},{"location":"attack-techniques/aws/ec2-steal-instance-credentials/#attacker-actions","title":"Attacker Actions:","text":"<ul> <li>Attempts to SSM into EC2 instance with defined user.  Once credentials are retrieved, the workflow then calls the API 'DescribeInstances' with the EC2 instance profile credentials from the proxy-app in Google CLoud (outside AWS IP space).</li> <li>Resulting event name: <code>DescribeInstances</code></li> <li>Assigned IAM Permission: <code>ec2:DescribeInstances</code></li> </ul>"},{"location":"attack-techniques/aws/ec2-steal-instance-credentials/#workflow-inputs","title":"Workflow Inputs:","text":"<p>Specify which user this attack should run as.  <pre><code>{\"user\":\"user01\"}\n{\"user\":\"user02\"}\n</code></pre></p>"},{"location":"attack-techniques/aws/ec2-steal-instance-credentials/#clean-up","title":"Clean Up:","text":"<p>None</p>"},{"location":"attack-techniques/aws/ec2-steal-instance-credentials/#execution-instructions","title":"Execution Instructions","text":"<ul> <li>See User Guide for Execution Instructions via the Google Cloud Console</li> <li>Programmatically execute this workflow with the following cli command:</li> </ul> <pre><code>gcloud workflows run aws-delete-cloudtrail-trail `--data={\"user\": \"user01\"}` \n</code></pre>"},{"location":"attack-techniques/aws/ec2-steal-instance-credentials/#detection-artifacts","title":"Detection Artifacts","text":"<p>GuardDuty provides two findings to identify stolen EC2 instance credentials.</p> <ul> <li> identifies EC2 instance credentials used from outside an AWS account. </li> <li> identifies EC2 instance credentials used from a different AWS account than the one of the EC2 instance.</li> </ul> <p>See also: .</p>"},{"location":"attack-techniques/aws/organizations-leave/","title":"Attempt to Leave the AWS Organization","text":"<p>Platform: AWS</p>"},{"location":"attack-techniques/aws/organizations-leave/#mitre-attck-tactics","title":"MITRE ATT&amp;CK Tactics","text":"<ul> <li>Defense Evasion</li> </ul>"},{"location":"attack-techniques/aws/organizations-leave/#description","title":"Description","text":"<p>Attempts to leave the AWS Organization.  Since the execution users are not granted this permission, the attempt will fail and result in an AccessDenied error.</p>"},{"location":"attack-techniques/aws/organizations-leave/#attacker-actions","title":"Attacker Actions:","text":"<ul> <li>Attempts to leave an AWS Organization.</li> <li>Resulting event name: <code>LeaveOrganization</code></li> <li>Assigned IAM Permission: <code>organizations:LeaveOrganization</code></li> </ul>"},{"location":"attack-techniques/aws/organizations-leave/#workflow-inputs","title":"Workflow Inputs:","text":"<p>Specify which user this attack should run as.  <pre><code>{\"user\":\"user01\"}\n{\"user\":\"user02\"}\n</code></pre></p>"},{"location":"attack-techniques/aws/organizations-leave/#clean-up","title":"Clean Up:","text":"<ul> <li>Recreates the CloudTrail trail.</li> <li>Executed as the <code>DeRF Default User</code></li> </ul>"},{"location":"attack-techniques/aws/organizations-leave/#execution-instructions","title":"Execution Instructions","text":"<ul> <li>See User Guide for Execution Instructions via the Google Cloud Console</li> <li>Programmatically execute this workflow with the following cli command:</li> </ul> <pre><code>gcloud workflows run aws-delete-cloudtrail-trail `--data={\"user\": \"user01\"}` \n</code></pre>"},{"location":"attack-techniques/aws/organizations-leave/#detection-artifacts","title":"Detection Artifacts","text":"<p>Any attempts from a child account to leave its AWS Organization should be considered suspicious as leaving the organization can remove security controls enforced from the Organizational Management Account</p>"},{"location":"attack-techniques/aws/rds-share-snapshot/","title":"Exfiltrate RDS Snapshot by Sharing","text":"<p>Platform: AWS</p>"},{"location":"attack-techniques/aws/rds-share-snapshot/#mitre-attck-tactics","title":"MITRE ATT&amp;CK Tactics","text":"<ul> <li>Exfiltration</li> </ul>"},{"location":"attack-techniques/aws/rds-share-snapshot/#description","title":"Description","text":"<p>This attack has two different ways to share a RDS DB Snapshot externally. - The first option shares a RDS DB Snapshot with \"all\",  aka everyone.  - The second case shares a RDS DB Snapshot with an external, fictitious AWS account, (012345678912)</p>"},{"location":"attack-techniques/aws/rds-share-snapshot/#attacker-actions","title":"Attacker Actions:","text":"<ul> <li>Calls the <code>ModifyDBSnapshotAttribute</code> API, adding the <code>restore</code> attribute and assigning that to the AWS Account, <code>012345678912</code> or <code>all</code></li> <li>Resulting event name: <code>ModifyDBSnapshotAttribute</code></li> <li>Assigned IAM Permission: <code>rds:ModifyDBSnapshotAttribute</code></li> </ul>"},{"location":"attack-techniques/aws/rds-share-snapshot/#workflow-inputs","title":"Workflow Inputs:","text":"<p>Specify which user this attack should run as.  <pre><code># Sharing the RDS DB Snapshot with \"all\", aka, publicly\n{\"case\":\"1\",\"user\":\"user01\"}\n{\"case\":\"1\",\"user\":\"user02\"}\n\n# Sharing the RDS DB Snapshot with an external fictitious account\n{\"case\":\"2\",\"user\":\"user01\"}\n{\"case\":\"2\",\"user\":\"user02\"}\n</code></pre></p>"},{"location":"attack-techniques/aws/rds-share-snapshot/#clean-up","title":"Clean Up:","text":"<ul> <li>Removes the external fictitious external AWS account or \"all\" from the <code>restore</code> attribute.</li> <li>Executed as the <code>DeRF Default User</code></li> </ul>"},{"location":"attack-techniques/aws/rds-share-snapshot/#execution-instructions","title":"Execution Instructions","text":"<ul> <li>See User Guide for Execution Instructions via the Google Cloud Console</li> <li>Programmatically execute this workflow with the following cli command:</li> </ul> <pre><code>gcloud workflows run aws-rds-share-snapshot-srt --data={\"case\":\"1\",\"user\":\"user01\"}   \n</code></pre>"},{"location":"attack-techniques/aws/rds-share-snapshot/#detection-artifacts","title":"Detection Artifacts","text":"<p>Identify when a RDS snapshot is shared with an external account, through CloudTrail's <code>ModifyDBSnapshotAttribute</code> event specifically when the <code>requestParameters.valuesToAdd</code> key either contains an external AWS Account or the string \"all\".</p> <p> </p> <p>Refer to Stratus Red Team documentation for additional detailed detection artifacts produced by this attack technique.</p>"},{"location":"attack-techniques/aws/secretsmanager-retrieve-secrets/","title":"AWS Retrieve a High Number of Secrets Manager secrets","text":"<p>Platform: AWS</p>"},{"location":"attack-techniques/aws/secretsmanager-retrieve-secrets/#mitre-attck-tactics","title":"MITRE ATT&amp;CK Tactics","text":"<ul> <li>Credential Access</li> </ul>"},{"location":"attack-techniques/aws/secretsmanager-retrieve-secrets/#description","title":"Description","text":"<p>Lists the secrets stored in Secrets Manager and retrieves (20) secret values</p>"},{"location":"attack-techniques/aws/secretsmanager-retrieve-secrets/#attacker-actions","title":"Attacker Actions:","text":"<ul> <li>First, lists the secrets stored in Secrets Manager in the current region.</li> <li>Secondly, retrieves the values of (20) secrets stored in Secrets Manager</li> <li>Resulting event names: <ul> <li><code>ListSecrets</code></li> <li><code>GetSecretValue</code></li> </ul> </li> <li>Assigned IAM Permission: <ul> <li><code>secretsmanager:ListSecrets</code></li> <li><code>secretsmanager:GetSecretValue</code></li> </ul> </li> </ul>"},{"location":"attack-techniques/aws/secretsmanager-retrieve-secrets/#workflow-inputs","title":"Workflow Inputs:","text":"<p>Specify which user this attack should run as.  <pre><code>{\"user\":\"user01\"}\n{\"user\":\"user02\"}\n</code></pre></p>"},{"location":"attack-techniques/aws/secretsmanager-retrieve-secrets/#clean-up","title":"Clean Up:","text":"<p>None</p>"},{"location":"attack-techniques/aws/secretsmanager-retrieve-secrets/#execution-instructions","title":"Execution Instructions","text":"<ul> <li>See User Guide for Execution Instructions via the Google Cloud Console</li> <li>Programmatically execute this workflow with the following cli command:</li> </ul> <pre><code>gcloud workflows run aws-delete-cloudtrail-trail `--data={\"user\": \"user01\"}` \n</code></pre>"},{"location":"attack-techniques/aws/secretsmanager-retrieve-secrets/#detection-artifacts","title":"Detection Artifacts","text":"<p>Identify principals retrieving a high number of secrets, through CloudTrail's GetSecretValue event.</p>"},{"location":"attack-techniques/aws/ssm-retrieve-securestring-parameters/","title":"AWS Retrieve and Decrypt SSM Parameters","text":"<p>Platform: AWS</p>"},{"location":"attack-techniques/aws/ssm-retrieve-securestring-parameters/#mitre-attck-tactics","title":"MITRE ATT&amp;CK Tactics","text":"<ul> <li>Credential Access</li> </ul>"},{"location":"attack-techniques/aws/ssm-retrieve-securestring-parameters/#description","title":"Description","text":"<p>Describes the SSM Parameters in an Account and retrieves and decrypts (30) SSM Parameters available in an AWS region.</p>"},{"location":"attack-techniques/aws/ssm-retrieve-securestring-parameters/#attacker-actions","title":"Attacker Actions:","text":"<ul> <li>First, lists the SSM Parameters in the current region.</li> <li>Secondly, retrieves the values of (30) SSM Parameters.</li> <li>Resulting event names: <ul> <li><code>DescribeParameters</code></li> <li><code>GetParameter</code></li> </ul> </li> <li>Assigned IAM Permission: <ul> <li><code>ssm:DescribeParameters</code></li> <li><code>ssm:GetParameter</code></li> </ul> </li> </ul>"},{"location":"attack-techniques/aws/ssm-retrieve-securestring-parameters/#workflow-inputs","title":"Workflow Inputs:","text":"<p>Specify which user this attack should run as.  <pre><code>{\"user\":\"user01\"}\n{\"user\":\"user02\"}\n</code></pre></p>"},{"location":"attack-techniques/aws/ssm-retrieve-securestring-parameters/#clean-up","title":"Clean Up:","text":"<p>None</p>"},{"location":"attack-techniques/aws/ssm-retrieve-securestring-parameters/#execution-instructions","title":"Execution Instructions","text":"<ul> <li>See User Guide for Execution Instructions via the Google Cloud Console</li> <li>Programmatically execute this workflow with the following cli command:</li> </ul> <pre><code>gcloud workflows run aws-delete-cloudtrail-trail `--data={\"user\": \"user01\"}` \n</code></pre>"},{"location":"attack-techniques/aws/ssm-retrieve-securestring-parameters/#detection-artifacts","title":"Detection Artifacts","text":"<p>Identify principals retrieving a high number of SSM Parameters, through the AWS <code>GetParameter</code> event. </p> <p>Refer to Stratus Red Team documentation for additional detailed detection artifacts produced by this attack technique.</p>"},{"location":"attack-techniques/aws/vpc-remove-flow-log/","title":"AWS Remove VPC Flow Logs","text":"<p>Platform: AWS</p>"},{"location":"attack-techniques/aws/vpc-remove-flow-log/#mitre-attck-tactics","title":"MITRE ATT&amp;CK Tactics","text":"<ul> <li>Defense Evasion</li> </ul>"},{"location":"attack-techniques/aws/vpc-remove-flow-log/#description","title":"Description","text":"<p>Removes a VPC Flog Logs configuration from a VPC.</p>"},{"location":"attack-techniques/aws/vpc-remove-flow-log/#attacker-actions","title":"Attacker Actions:","text":"<ul> <li>Deletes a VPC Flow Log from a VPC.</li> <li>Resulting event name: <code>DeleteFlowLogs</code></li> <li>Assigned IAM Permission: <code>ec2:DeleteFlowLogs</code></li> </ul>"},{"location":"attack-techniques/aws/vpc-remove-flow-log/#workflow-inputs","title":"Workflow Inputs:","text":"<p>Specify which user this attack should run as.  <pre><code>{\"user\":\"user01\"}\n{\"user\":\"user02\"}\n</code></pre></p>"},{"location":"attack-techniques/aws/vpc-remove-flow-log/#clean-up","title":"Clean Up:","text":"<ul> <li>Recreates the VPC Flow Log configuration.</li> <li>Executed as the <code>DeRF Default User</code></li> </ul>"},{"location":"attack-techniques/aws/vpc-remove-flow-log/#execution-instructions","title":"Execution Instructions","text":"<ul> <li>See User Guide for Execution Instructions via the Google Cloud Console</li> <li>Programmatically execute this workflow with the following cli command:</li> </ul> <pre><code>gcloud workflows run aws-delete-cloudtrail-trail `--data={\"user\": \"user01\"}` \n</code></pre>"},{"location":"attack-techniques/aws/vpc-remove-flow-log/#detection-artifacts","title":"Detection Artifacts","text":"<p>Use the Cloudtrail event <code>DeleteFlowLogs</code> to identify activity.   Refer to Stratus Red Team documentation for additional detailed detection artifacts produced by this attack technique.</p>"},{"location":"attack-techniques/gcp/","title":"GCP","text":"<p>This page contains The DeRF attack techniques for available for GCP.</p>"},{"location":"attack-techniques/gcp/#privilege-escalation","title":"Privilege Escalation","text":"<ul> <li>Impersonate GCP Service Accounts</li> </ul>"},{"location":"attack-techniques/gcp/#exfiltration","title":"Exfiltration","text":"<ul> <li>Share Compute Disk</li> <li>Exfiltrate Data from BigQuery Table via Unauthorized Query</li> </ul>"},{"location":"attack-techniques/gcp/#persistence","title":"Persistence","text":"<ul> <li>Backdoor a GCP Service Account through its IAM Policy</li> </ul>"},{"location":"attack-techniques/gcp/backdoor-service-account/","title":"Backdoor a GCP Service Account through its IAM Policy","text":"<p>Platform: GCP</p>"},{"location":"attack-techniques/gcp/backdoor-service-account/#mitre-attck-tactics","title":"MITRE ATT&amp;CK Tactics","text":"<ul> <li>Persistence</li> </ul>"},{"location":"attack-techniques/gcp/backdoor-service-account/#description","title":"Description","text":"<p>This attack technique adds an IAM Role Binding to a Service Account granting access to a User outside the Google Organization, mimicking the 'backdooring' of a service account.  This could also be considered granting 'external access'.</p>"},{"location":"attack-techniques/gcp/backdoor-service-account/#attacker-actions","title":"Attacker Actions","text":"<ul> <li> <p>Attempt to setIAMPolicy on a Service Account, granting the Service Account User role to a User account outside the Organization. Because the User Account must exist, the attack technique grants access to the same valid account that Stratus Red Team does: <code>stratusredteam@gmail.com</code> </p> </li> <li> <p>Resulting event name: <code>google.iam.admin.v1.SetIAMPolicy</code></p> </li> <li>Assigned IAM Permission: <code>iam.serviceAccounts.setIamPolicy</code></li> </ul>"},{"location":"attack-techniques/gcp/backdoor-service-account/#workflow-inputs","title":"Workflow Inputs","text":"<p>Specify which DeRF attacker service account this attack should run as.  <pre><code>{\"sa\":\"01\"}\n{\"sa\":\"02\"}\n</code></pre></p>"},{"location":"attack-techniques/gcp/backdoor-service-account/#clean-up","title":"Clean Up:","text":"<p>None</p>"},{"location":"attack-techniques/gcp/backdoor-service-account/#execution-instructions","title":"Execution Instructions","text":"<ul> <li>See User Guide for Execution Instructions via the Google Cloud Console</li> <li>Programmatically execute this workflow with the following cli command:</li> </ul> <pre><code>gcloud workflows run gcp-impersonate-sa-srt `--data={\"sa\": \"01\"}` \n</code></pre>"},{"location":"attack-techniques/gcp/backdoor-service-account/#detection-artifacts","title":"Detection Artifacts","text":"<p>Refer to Stratus Red Team documentation for detailed detection artifacts produced by this attack technique.</p> <p></p>"},{"location":"attack-techniques/gcp/backdoor-service-account/#organization-policy-constraints","title":"Organization Policy Constraints","text":"<p>An organization policy is a configuration of restrictions which can be applied at several levels of your Google Cloud resource hierarchy. </p> <p>This attack technique can be prevented with the <code>Domain restricted sharing</code> constraint by making use of the <code>constraints/iam.allowedPolicyMemberDomains</code> list by confining who can be granted IAM Policy to a defined list of domains.   </p> <p>See Google Documentation for a complete list of Organization Policy Constraints available.</p>"},{"location":"attack-techniques/gcp/bq-data-exfiltration-via-job/","title":"Exfiltrate Data from BigQuery Table via Unauthorized Query","text":"<p>Platform: GCP</p>"},{"location":"attack-techniques/gcp/bq-data-exfiltration-via-job/#mitre-attck-tactics","title":"MITRE ATT&amp;CK Tactics","text":"<ul> <li>Exfiltration</li> </ul>"},{"location":"attack-techniques/gcp/bq-data-exfiltration-via-job/#description","title":"Description","text":"<p>SQL queries in BigQuery operate asynchronously through a job submission process, where the results are queried afterward. This attack technique involves executing a query on a BigQuery Table to retrieve all data from every column.</p>"},{"location":"attack-techniques/gcp/bq-data-exfiltration-via-job/#attacker-actions","title":"Attacker Actions","text":"<p>The attack technique first calls the googleapis.bigquery.v2.jobs.insert REST API, submitting a SQL query selecting all data from the <code>derf-target-dev.derf_dataset.derf_table1</code> BigQuery Table.  </p> <ul> <li>Log methodName : <code>jobservice.insert</code></li> <li>Required Permissions: <code>bigquery.jobs.create</code> and <code>bigquery.tables.getData</code></li> </ul> <p>Secondly, the attack technique calls the googleapis.bigquery.v2.jobs.getQueryResults REST API, returning the result of the previously submitted SQL query using the JobId as reference.</p> <ul> <li>Log methodName : <code>jobservice.getqueryresults</code></li> <li>Required Permissions: <code>bigquery.jobs.create</code></li> </ul>"},{"location":"attack-techniques/gcp/bq-data-exfiltration-via-job/#workflow-inputs","title":"Workflow Inputs","text":"<p>None. The workflow will always run as Attack Execution Service Account 01 <code>derf-attacker-sa-01@PROJECT_ID.iam.gserviceaccount.com</code></p>"},{"location":"attack-techniques/gcp/bq-data-exfiltration-via-job/#clean-up","title":"Clean Up:","text":"<p>None</p>"},{"location":"attack-techniques/gcp/bq-data-exfiltration-via-job/#execution-instructions","title":"Execution Instructions","text":"<ul> <li>See the User Guide for Execution Instructions via the Google Cloud Console</li> <li>Programmatically execute this workflow with the following cli command:</li> </ul> <pre><code>gcloud workflows run gcp-bq-data-exfilration-via-job-toc\n</code></pre>"},{"location":"attack-techniques/gcp/bq-data-exfiltration-via-job/#detection-artifacts","title":"Detection Artifacts","text":"<p>LogName: <code>projects/-/logs/cloudaudit.googleapis.com/data_access</code> Producer: <code>bigquery.googleapis.com</code> </p> <p>Run SQL query with a Job: <code>googleapis.bigquery.v2.jobs.insert</code> </p> <p>Retrieve SQL query results: <code>googleapis.bigquery.v2.jobs.getQueryResults</code> </p> <p></p>"},{"location":"attack-techniques/gcp/bq-data-exfiltration-via-job/#control-objectives","title":"Control Objectives","text":"<p>Refer to the TrustOnCloud Control Catalog Dashboard for a complete list of controls and control objectives mapped to this attack technique.</p>"},{"location":"attack-techniques/gcp/impersonate-service-accounts/","title":"Impersonate GCP Service Accounts","text":"<p>Platform: GCP</p>"},{"location":"attack-techniques/gcp/impersonate-service-accounts/#mitre-attck-tactics","title":"MITRE ATT&amp;CK Tactics","text":"<ul> <li>Privilege Escalation</li> </ul>"},{"location":"attack-techniques/gcp/impersonate-service-accounts/#description","title":"Description","text":"<p>Attempts to impersonate 10 different GCP service accounts in the project. Service account impersonation in GCP is the retrieval temporary credentials (OAuth bearer tokens) allowing the impersonator to 'act as' the targeted service account.</p>"},{"location":"attack-techniques/gcp/impersonate-service-accounts/#attacker-actions","title":"Attacker Actions:","text":"<ul> <li>Attempt to impersonate each of the 10 service accounts created for this detection. Only one impersonation request will succeed, simulating a successful privilege escalation.</li> <li>Resulting event name: <code>GenerateAccessToken</code></li> <li>Assigned IAM Permission: <code>iam.serviceAccounts.actAs</code></li> </ul>"},{"location":"attack-techniques/gcp/impersonate-service-accounts/#workflow-inputs","title":"Workflow Inputs:","text":"<p>Specify which derf attacker service account this attack should run as.  <pre><code>{\"sa\":\"01\"}\n{\"sa\":\"02\"}\n</code></pre></p>"},{"location":"attack-techniques/gcp/impersonate-service-accounts/#clean-up","title":"Clean Up:","text":"<p>None</p>"},{"location":"attack-techniques/gcp/impersonate-service-accounts/#execution-instructions","title":"Execution Instructions","text":"<ul> <li>See User Guide for Execution Instructions via the Google Cloud Console</li> <li>Programmatically execute this workflow with the following cli command:</li> </ul> <pre><code>gcloud workflows run gcp-impersonate-sa-srt `--data={\"sa\": \"01\"}` \n</code></pre>"},{"location":"attack-techniques/gcp/impersonate-service-accounts/#detection-artifacts","title":"Detection Artifacts","text":"<p>Using GCP Admin Activity audit logs event <code>GenerateAccessToken</code>. This event is not included in default logging and needs to be enabled. Specifically, IAM data access activity logs need to be enabled.  The principal caller is recorded in the log whether the event was a success or resulted in an error.</p> <p></p>"},{"location":"attack-techniques/gcp/share-compute-disk/","title":"Exfiltrate Compute Disk by sharing it","text":"<p>Platform: GCP</p>"},{"location":"attack-techniques/gcp/share-compute-disk/#mitre-attck-tactics","title":"MITRE ATT&amp;CK Tactics","text":"<ul> <li>Exfiltration</li> </ul>"},{"location":"attack-techniques/gcp/share-compute-disk/#description","title":"Description","text":"<p>This attack technique shares a compute disk with a ficticious GCP Project.  The attacker could then create a snapshot of the disk in their GCP project.</p>"},{"location":"attack-techniques/gcp/share-compute-disk/#attacker-actions","title":"Attacker Actions:","text":"<ul> <li>Attempt to update the IAM policy of a compute disk granting access to a User outside of the Google Cloud Organization.</li> <li>Resulting event name: <code>v1.compute.disks.setIamPolicy</code></li> <li>Assigned IAM Permission: <code>compute.disks.setIamPolicy</code></li> </ul>"},{"location":"attack-techniques/gcp/share-compute-disk/#workflow-inputs","title":"Workflow Inputs:","text":"<p>Specify which derf attacker service account this attack should run as.  <pre><code>{\"sa\":\"01\"}\n{\"sa\":\"02\"}\n</code></pre></p>"},{"location":"attack-techniques/gcp/share-compute-disk/#clean-up","title":"Clean Up:","text":"<p>None</p>"},{"location":"attack-techniques/gcp/share-compute-disk/#execution-instructions","title":"Execution Instructions","text":"<ul> <li>See User Guide for Execution Instructions via the Google Cloud Console</li> <li>Programmatically execute this workflow with the following cli command:</li> </ul> <pre><code>gcloud workflows run gcp-impersonate-sa-srt `--data={\"sa\": \"01\"}` \n</code></pre>"},{"location":"attack-techniques/gcp/share-compute-disk/#detection-artifacts","title":"Detection Artifacts","text":"<p>Refer to Stratus Red Team documentation for detailed detection artifacts produced by this attack technique.</p> <p></p>"},{"location":"attack-techniques/gcp/share-compute-disk/#organization-policy-constraints","title":"Organization Policy Constraints","text":"<p>An organization policy is a configuration of restrictions which can be applied at several levels of your Google Cloud resource hierarchy. </p> <p>This attack technique can be prevented with the <code>Domain restricted sharing</code> constraint by making use of the <code>constraints/iam.allowedPolicyMemberDomains</code> list by confining who can be granted IAM Policy to a defined list of domains.   </p> <p>See Google Documentation for a complete list of Organization Policy Constraints available.</p>"},{"location":"user-guide/attack-execution-access-control/","title":"End User Execution Permissions","text":""},{"location":"user-guide/attack-execution-access-control/#attack-execution-access-control","title":"Attack Execution - Access Control","text":"<p>The ability to execute an attack corresponds to the ability to invoke Cloud Workflows in your DeRF GCP Project.</p>"},{"location":"user-guide/attack-execution-access-control/#roles","title":"Roles","text":""},{"location":"user-guide/attack-execution-access-control/#execute-attacks-only","title":"Execute Attacks Only","text":"<ul> <li>Only the Workflows Invoker Role is needed to invoke the workflows and subsequently execute an attack.<ul> <li>roles/workflows.invoker</li> </ul> </li> </ul>"},{"location":"user-guide/attack-execution-access-control/#execute-an-attack-and-triage-issues","title":"Execute an Attack AND Triage issues","text":"<ul> <li>Only the Workflows Invoker Role is needed to invoke the workflows and subsequently execute an attack.<ul> <li>roles/workflows.invoker</li> </ul> </li> <li>Additional ReadOnly Roles required to give visibility into the underlying infrastructure, view logs, etc<ul> <li>roles/run.viewer</li> <li>roles/cloudbuild.builds.viewer</li> <li>roles/logging.viewer</li> </ul> </li> </ul>"},{"location":"user-guide/attack-execution-access-control/#best-practices-for-role-assignment-in-gcp","title":"Best Practices for Role Assignment in GCP","text":"<p>Its best practice to assign the above clusters of Roles to groups rather than individual users or service accounts.    </p> <pre><code>- If Google Workspace is your primary Identity Provider, create a group and assign membership under 'Directory -&gt; Groups'. Once created in Google Workspace, your groups for Attack Execution will be available to assign Roles.   \n- If federating Google Workspace against another Identity Provider, create a group and assign membership in your Identity Provider. Sync the group and its members from your Identity Provider to Google Workspace with automatic SCIM provisioning.  Once populated in Google Workspace, your groups for Attack Execution will be available to assign Roles.    \n- If using Cloud Identity, from the cloud console, navigate to the [Groups](https://console.cloud.google.com/iam-admin/groups) page.  Create a group and assign membership.  Once created, your groups for Attack Execution will be available to assign Roles.\n</code></pre>"},{"location":"user-guide/aws-attack-creation/","title":"AWS Attack Technique Creation","text":""},{"location":"user-guide/aws-attack-creation/#building-your-own-aws-attack","title":"Building your own AWS Attack","text":"<p>Its possible as your use cases grow, you will want to expand on the library of built-in attack techniques and create your own custom modules.  Follow this guide for working with The DeRF to create your own workflows that execute as AWS Attack Techniques</p> <ol> <li> <p>Fork the <code>derf</code> repo found here.</p> </li> <li> <p>From the top-level directory, <code>attacks-internal</code>, review the <code>sample-attack</code> directory for an example of the structure of an AWS attack module.  Every attack module should be a folder containing at least the following files:   </p> <ul> <li><code>attack.tf</code>: The Google Workflow, defined in terraform, which outlines the API calls to make in the attack sequence</li> <li><code>iam-permissions.tf</code>: Any additional permissions need for the DeRF Execution Users to perform the attack.  Refer to the <code>sample-attack</code> directory for an example of the resources to create.</li> <li><code>variable.tf</code>: Refer to the <code>sample-attack</code> directory for the common variable imported into every attack.</li> <li><code>infra.tf</code>: If the attack technique requires any new target infrastructure, define it in this file.</li> </ul> </li> <li> <p>Create your new custom DeRF attack technique as a new folder in the <code>attacks-internal</code> directory.</p> </li> <li> <p>From the top-level directory, <code>env-prod</code>, review the <code>aws-attack-techniques-internal.tf</code> file.  From this file, source the newly created attack module in the style of the <code>sample-attack-module</code>.</p> </li> <li> <p>Re-deploy the infrastructure  following instructions here including the re-initalizing of terraform.</p> </li> </ol>"},{"location":"user-guide/aws-attack-creation/#specifying-details-of-an-aws-attack-technique","title":"Specifying Details of an AWS Attack Technique","text":"<p>Details of every API call to AWS is specified in the Google Workflows in the <code>http.post</code> request and passed to the <code>aws-proxy-app</code> for processing.  Below is a detailed accounting of the variables which can be sent to the <code>aws-proxy-app</code> to detail the API call to AWS.</p>"},{"location":"user-guide/aws-attack-creation/#variables","title":"Variables","text":"<ul> <li>HOST:   <ul> <li>description:   The value of the <code>Host</code> HTTP header to send with the API request.   Most frequently constructed as: servicename.region.amazonaws.com</li> <li>example: cloudtrail.us-east-1.amazonaws.com</li> <li>required: yes</li> </ul> </li> <li>REGION: <ul> <li>description:  Region the target infrastructure is located. </li> <li>example: us-east-1</li> <li>required: yes</li> </ul> </li> <li>SERVICE: <ul> <li>description: Name of the service the targeted API belongs to.</li> <li>example: cloudtrail</li> <li>required: yes</li> </ul> </li> <li>ENDPOINT: <ul> <li>description: The full URL of the API call. Commonly \u201chttps:// + host header value\u201d</li> <li>example: https://cloudtrail.us-east-1.amazonaws.com</li> <li>required: yes</li> </ul> </li> <li>VERB: <ul> <li>description: HTTP verb to send the request as.</li> <li>example: POST</li> <li>required: yes</li> </ul> </li> <li>BODY: <ul> <li>description: If POST or PUT request, the Body of the HTTP request to send.</li> <li>example: '{\"Name\": \"derf-trail\"}'</li> <li>required: no</li> </ul> </li> <li>UA: <ul> <li>description: The value of the <code>User-Agent</code> HTTP header to send with the API request.  Using the pattern below, they are recorded as unique per workflow execution, helpful in identifying attack executions within logs. </li> <li>example: '$${\"Derf-AWS-Delete-CloudTrail==\"+sys.get_env(\"GOOGLE_CLOUD_WORKFLOW_EXECUTION_ID\")}'</li> <li>required: no</li> </ul> </li> <li>CONTENT: <ul> <li>description: The value of the <code>Content-Type</code> HTTP header to send with the API request.</li> <li>example: \"application/x-amz-json-1.1\"</li> <li>required: no</li> </ul> </li> <li>USER: <ul> <li>description: The DeRF Execution User (either 01 or 02) to run the attack as    </li> <li>example: $${user}</li> <li>required: no</li> </ul> </li> <li>TARGET: <ul> <li>description: The value of the <code>X-Amz-Target</code> HTTP header to send with the API request. Some AWS APIs require this HTTP header to interface with the API. Proxy and record API calls from your AWS CLI traffic to understand if the API you are working with requires this header.         </li> <li>example: com.amazonaws.cloudtrail.v20131101.CloudTrail_20131101.DeleteTrail</li> <li>required: no</li> </ul> </li> <li>TEMPCREDSPASSED: <ul> <li>description: When \u2018yes\u2019, indicates to the downstream aws proxy application to except credentials sent in the HTTP header and to run the attack as.    </li> <li>example: yes</li> <li>required: no</li> </ul> </li> <li>MD5: <ul> <li>description: When \u2018yes\u2019, indicates to the downstream aws proxy application to calculate the MD5 hash of the post body parameters and include as a signed header. Only used in very specific AWS API calls such as <code>PutBucketLifecycleConfiguration</code>.    </li> <li>example: yes</li> <li>required: no</li> </ul> </li> <li>ACCESSKEYID, ACCESSKEYSECRET &amp; SESSIONTOKEN:<ul> <li>description: When included in the Google Workflow request, these temporary credentials are used by the application to run the attack as.    </li> <li>example:<ul> <li>ACCESSKEYID: '$${ACCESSKEYID}'</li> <li>ACCESSKEYSECRET: '$${ACCESSKEYSECRET}'</li> <li>SESSIONTOKEN: '$${SESSIONTOKEN}'</li> </ul> </li> <li>required: no</li> </ul> </li> </ul>"},{"location":"user-guide/aws-attack-creation/#sample-google-workflow-step","title":"Sample Google Workflow Step","text":"<pre><code>    - DeleteTrail:\n        call: http.post\n        args:\n          url: '$${appEndpoint+\"/submitRequest\"}'\n          auth:\n              type: OIDC\n          headers:\n            Content-Type: application/json\n          body:\n              HOST: cloudtrail.us-east-1.amazonaws.com\n              REGION: \"us-east-1\"\n              SERVICE: \"cloudtrail\" \n              ENDPOINT: \"https://cloudtrail.us-east-1.amazonaws.com\"\n              BODY: '{\"Name\": \"derf-trail\"}'\n              UA: '$${\"Derf-AWS-Delete-CloudTrail==\"+sys.get_env(\"GOOGLE_CLOUD_WORKFLOW_EXECUTION_ID\")}'\n              CONTENT: \"application/x-amz-json-1.1\"\n              USER: $${user}\n              VERB: POST\n              TARGET: com.amazonaws.cloudtrail.v20131101.CloudTrail_20131101.DeleteTrail\n        result: response\n</code></pre>"},{"location":"user-guide/execution-user-permissions/","title":"DeRF Execution User Permissions","text":""},{"location":"user-guide/execution-user-permissions/#derf-execution-permissions","title":"DeRF Execution Permissions","text":"<p>The permissions assigned to the <code>derf-execution-users</code> or the <code>DeRF-Default-User</code> are NOT documented here.  Rather, this is a guide as to where you can find those permissions and how to update them.</p>"},{"location":"user-guide/execution-user-permissions/#execution-user-group-membership","title":"Execution User Group Membership","text":"<p>Both User 01 and 02 are members of the <code>derf-execution-users</code> group, allowing them to perform all the same attack techniques outlined in the DeRF.</p>"},{"location":"user-guide/execution-user-permissions/#derf-execution-user-policy-assignments","title":"DeRF Execution User - Policy Assignments","text":"<p>Every attack technique is responsible for creating a policy containing the permissions needed to execute the attack and assigning it to the  <code>derf-execution-users</code> group. The policy and group assignments are found in the <code>iam-permissions.tf</code> file within the <code>/attack-techniques/aws/permissions-required</code> module.   </p>"},{"location":"user-guide/execution-user-permissions/#derf-default-user-policy-assignments","title":"DeRF Default User - Policy Assignments","text":"<p>If an attack technique needs the default user to reverse a state changing action or perform another task that should not be attributed to the execution users, the module is responsible for creating a policy containing the necessary permissions and attaching it to the <code>DeRF-Default-User</code>. The policy and group assignments are found in the <code>iam-permissions.tf</code> file within the <code>/attack-techniques/aws/permissions-required</code> module.    </p>"},{"location":"user-guide/getting-started/","title":"Getting Started","text":""},{"location":"user-guide/getting-started/#step-1-deployment","title":"Step 1 - Deployment","text":"<p>Ensure you have deployed the DeRF resources.  The deployment includes resources in both a targeted AWS Account and the DeRF Framework components housed in a GCP Project.  See the Deployment Guide for detailed instructions.</p>"},{"location":"user-guide/getting-started/#step-2-attack-access-control","title":"Step 2 - Attack Access Control","text":"<p>The ability to execute an attack corresponds to the ability to invoke Cloud Workflows in your DeRF GCP Project. See the Attack Execution Access Control Guide for detailed instructions on how to assign permissions to End Users for attack execution and troubleshooting.</p>"},{"location":"user-guide/getting-started/#step-3-attack-execution","title":"Step 3 - Attack Execution","text":"<p>Attack Techniques are codified as Google Cloud Workflows.  They can be executed as one of two predefined Users and from either the Google Cloud Console or programmatically with the <code>gcloud cli</code>.         - See detailed instructions for executing the attacks from the Google Cloud Console    - See detailed instructions for executing the attacks with the <code>gcloud cli</code></p>"},{"location":"user-guide/programmatic-usage/","title":"Programmatic Usage","text":"<ol> <li>Ensure the Google command line tool is installed on your local system.  Reference Google maintained documentation for instructions on installing <code>gcloud cli</code></li> <li>Authenticate to Google Cloud Project which DeRF is deployed. <pre><code>gcloud auth login --project PROJECT-ID\n</code></pre></li> <li> <p>Invoke a particular attack techniques' workflow with the <code>gcloud cli</code>. See Google documentation for more comprehensive instructions on the workflows service.</p> </li> </ol>"},{"location":"user-guide/programmatic-usage/#aws","title":"AWS","text":"<pre><code>gcloud workflows run WORKFLOW-NAME `--data={\"user\": \"user01\"}` \n</code></pre>"},{"location":"user-guide/programmatic-usage/#gcp","title":"GCP","text":"<pre><code>gcloud workflows run WORKFLOW-NAME `--data={\"sa\": \"01\"}` \n</code></pre>"},{"location":"user-guide/troubleshooting/","title":"Troubleshooting","text":""},{"location":"user-guide/troubleshooting/#troubleshooting-derf-deployment","title":"Troubleshooting DeRF Deployment","text":"<pre><code> - You must be authenticated to AWS (and GCP) before deploying the DeRF via Terraform.\n</code></pre> <p><code>bash aws sso login --profile PROFILE-NAME</code></p> <pre><code> - You must set the AWS_PROFILE to the AWS profile of the target infrastructure\n</code></pre> <p><code>bash export AWS_PROFILE=PROFILE</code></p> <p>\"Error: Attempted to load application default credentials since neither credentials nor access_token was set in the provider block. No credentials loaded. To use your gcloud credentials, run 'gcloud auth application-default login' \"</p> <p></p> <pre><code>- When using the Google Cloud provider in terraform, be sure to generate 'ADC' credentials' with the following command:\n</code></pre> <p><code>gcloud auth application-default login --project=PROJECT_ID</code></p>"},{"location":"user-guide/troubleshooting/#cli-error-message-you-are-not-authenticated-against-aws","title":"CLI Error Message: \"You are not authenticated against AWS\"","text":"<p>\"You are not authenticated against AWS, or you have not set your region.\"  </p> <ul> <li>You must be authenticated to AWS (and GCP) before deploying the DeRF via Terraform. <pre><code>aws sso login --profile PROFILE_NAME\n</code></pre></li> </ul>"},{"location":"user-guide/troubleshooting/#cli-error-message-error-error-configuring-s3-backend","title":"CLI Error Message: \"Error: error configuring S3 Backend\"","text":"<p>Error: error configuring S3 Backend: no valid credential sources for S3 Backend found. \u2502 \u2502 Please see https://www.terraform.io/docs/language/settings/backends/s3.html \u2502 for more information about providing credentials. \u2502 \u2502 Error: SSOProviderInvalidToken: the SSO session has expired or is invalid</p>"},{"location":"user-guide/troubleshooting/#cli-error-message-error-failed-to-read-variables-file","title":"CLI Error Message: \"Error: Failed to read variables file\"","text":"<p>\"\u2502 Error: Failed to read variables file\"</p> <ul> <li>When running <code>terraform apply -var-file=derf.tfvars</code> the program must be to find the specified variables file. Ensure you are in the <code>./env-prod</code> directory when applying the terraform. Ensure the <code>.tfvars</code> file you specified is in your path.</li> </ul>"},{"location":"user-guide/troubleshooting/#cli-error-message-error-attempted-to-load-application-default-credentials","title":"CLI Error Message: \"Error: Attempted to load application default credentials\"","text":""},{"location":"user-guide/troubleshooting/#troubleshooting-attack-execution","title":"Troubleshooting Attack Execution","text":"<ol> <li>Error Message on the Google Cloud Console: <p>KeyError: key not found: user</p> </li> </ol> <p></p> <ul> <li>All workflows need to be executed with either User01 or User02. Do so by sending JSON input during workflow execution.  <ul> <li>Input Required:<ul> <li>{\"user\":\"user01\"} OR {\"user\":\"user02\"}</li> </ul> </li> </ul> </li> </ul>"},{"location":"user-guide/usage/","title":"Executing An Attack","text":"<p>Attacks execution is performed by invoking a Google Cloud Workflow. Workflows can be invoked either on the Google Cloud Console or programmatically with the <code>gcloud cli</code></p>"},{"location":"user-guide/usage/#attack-execution-on-the-console","title":"Attack Execution -  on the Console","text":"<ol> <li>Log into the Google Cloud Console and navigate to the workflows page.</li> <li>Click on the name of the workflow that matches the attach you want to execute. </li> <li>Click on the execute button. </li> <li>Refer to the code panel on the right-hand side and select which user to run the attack as by copying one of the possible inputs. </li> <li>Paste selected json in the input panel on the left-hand side. </li> <li>Finally, select the <code>Execute</code> button at the bottom of the screen.  The results of the attack will be displayed on the right-hand side of the screen.</li> </ol>"}]}