{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DeRF Documentation Home","text":"<p>Welcome to the Home Page of the DeRF Documentation.</p>"},{"location":"#high-level-architecture","title":"High Level Architecture","text":"<p>The DeRF\u2019s unique architecture is wholly deployed via terraform.  It consists of resources spread across AWS and GCP.</p>"},{"location":"#derf-attack-architecture-for-aws","title":"DeRF Attack Architecture for AWS","text":""},{"location":"#derf-user-personas","title":"DeRF User Personas","text":"<p>See the User Guide for more detailed descriptions of the permissions assigned to the DeRF Execution and Default Users.</p> <p> </p> <p>The DeRF Deployment User deploys the DeRF terraform module across AWS and GCP. Permissions required for the DeRF Deployment User are documented here </p> <p></p> <p> </p> <p>The DeRF Execution User 01 is one of two built in an AWS IAM Users which AS attack techniques can run as. Permissions are assigned to the <code>derf-execution-users</code> AWS IAM Group and documented within each attack module. </p> <p> </p> <p>The DeRF Execution User 02 is one of two built in an AWS IAM Users which AWS attack techniques can run as. Permissions are assigned to the <code>derf-execution-users</code> AWS IAM Group and documented within each attack module. </p> <p> </p> <p>The DeRF Default User is an AWS IAM User used by attack techniques to revert state changing actions by the AWS attack modules.  If AWS attack techniques are run with the user parameter left blank, the attack with default to run as this user.  </p> <p> </p> <p>In order to perform an attack as an arbitrary AWS Role, AWS IAM Temporary Session Credentials generated from IAM Roles can be passed directly to the <code>aws-proxy-app</code> as Post Body Parameters additionally with the <code>TEMPCREDSPASSED = yes</code> Post Body parameter. </p>"},{"location":"comparison/","title":"Comparison With Other Tools","text":"<p>The DeRF, an open-source tool available on GitHub, consisting of Terraform modules and a Cloud Run application written in Python. Within this package, a variety of attack techniques are provided, aligned with the capabilities of the Stratus Red Team. The decision to release the DeRF to have attack technique parity with Stratus Red Team is aimed at eliminating the need for operators to compromise between different attack capabilities. Instead, they can select the tooling that best suits their specific use case instead of the tooling which maintains a particular attack module.</p>"},{"location":"comparison/#stratus-red-team-by-data-dog","title":"Stratus Red Team by Data Dog","text":"<p>Stratus Red Team fashions itself as \"Atomic Red Team\u2122\", but focused on cloud.</p> <p>Stratus Red Team is a self-contained GO binary that can be used to detonate offensive attack techniques against a live cloud environments (AWS, GCP and Azure). It consists of a CLI tool which operators can 'detonate' individual attack techniques against an AWS, GCP or Azure target.  Each attack technique is a self-contained module which creates the infrastructure required for the attack in a warm-up phase, following which the attack is performed.  Finally, any created infrastructure is destroyed. While Stratus Red Team is an awesome tool for an individual operator, its not great for those less technical or when you need to democratize attack execution, making it invocation available to larger teams. </p> <p>DeRF has made the decision to release a predefined set of AWS attack techniques that aligns with the capabilities of Stratus Red Team. This choice ensures consistency among the publicly available tools. However, it's important to note that DeRF is designed to be extensible. Users are encouraged to develop their own modules, allowing for customization and expansion based on their specific requirements.</p> <p>Use Stratus Red Team when: There is an individual, technical operator who needs to execute a set of pre-defined attack techniques in AWS, Azure, K8s or GCP.</p> <p>Use DeRF when:  1. There are a group of individuals who needs to execute attack techniques in AWS. Especially consider the use of DeRF when the End User is less technical or attacks need to be automated and automation can easily authenticate against Google Cloud. 2. Or when you need to extend a tool, creating your own attack sequences.</p>"},{"location":"comparison/#atomic-red-team-by-red-canary","title":"Atomic Red Team by Red Canary","text":"<p>Credit: Description by Status Red Team</p> <p>Atomic Red Team\u2122 is library of tests mapped to the MITRE ATT&amp;CK\u00ae framework. Security teams can use Atomic Red Team to quickly, portably, and reproducibly test their environments.</p> <p>In 2021, Atomic Red Team added support for Cloud TTPs. In the summer 2022, Atomic Red Team also started leveraging Stratus Red Team to execute some of its cloud attack techniques.</p> <p>Atomic Red Team has very few cloud TTPs it implements itself. While Atomic Red Team is an awesome tool for endpoint security, it wasn't built purposely for cloud environments. In particular, it doesn't handle the prerequisite infrastructure and configuration necessary to detonate TTPs, and leaves that to the user.  For instance, AWS - Create Access Key and Secret Key requires you to create an IAM user prior to detonating the attack. Stratus Red Team packages this prerequisite logic, so you can detonate attack techniques without having to create any infrastructure or cloud configuration manually.</p> <p>However, the attack technique format of Atomic Red Team is based on YAML, and it's therefore easier to add new TTPs, even if they are not in the core of Atomic Red Team.</p>"},{"location":"comparison/#leonidas-by-f-secure-nick-jones","title":"Leonidas by F-Secure (Nick Jones)","text":"<p>Credit: Description by Status Red Team</p> <p>Leonidas is a framework for executing attacker actions in the cloud. It provides a YAML-based format for defining cloud attacker tactics, techniques and procedures (TTPs) and their associated detection properties</p> <p>While Stratus Red Team and Leonidas have similar goals, their implementation is fundamentally different.</p> <ul> <li>Leonidas is a fully-fledged web application you deploy in your AWS account using Terraform, and then a CodePipeline pipeline.</li> <li>Then, you use \"Leo\", the test case orchestrator, to hit the web API and detonate attack techniques. </li> <li>Leonidas allows describing TTPs as YAML, making it easier to extend than Stratus Red Team. </li> <li>Leonidas does not handle prerequisites for detonating attack techniques.</li> <li>The attack techniques implemented by Leonidas are very granular, meaning it can be challenging to implement detection for them. See for instance: STS Get Caller Identity</li> <li>Leonidas comes with a set of suggested threat detection rules. However, as its attack techniques are very granular, it is practically impossible to use them as-is in a real production environment, as they would trigger many false positives.</li> </ul> <p>Stratus Red Team aims at being simpler to use (single binary) and does not require you to have prior infrastructure or configuration in your AWS account. Stratus Red Team focuses on a single thing: executing cloud attack tactics against a live environment, with minimal overhead. You can also use Stratus Red Team programmatically, from Go code, as a library.</p>"},{"location":"comparison/#pacu-by-rhino-security-labs","title":"Pacu by Rhino Security Labs","text":"<p>Credit: Description by Status Red Team </p> <p>Pacu is an open-source AWS exploitation framework, designed for offensive security testing against cloud environments. Created and maintained by Rhino Security Labs, Pacu allows penetration testers to exploit configuration flaws within an AWS account, using modules to easily expand its functionality.</p> <p>Pacu is an offensive AWS exploitation framework, aimed at penetration testers. It implements various enumeration and exploitation methods, some straightforward and some advanced. For instance, lambda__backdoor_new_roles creates a Lambda function and a CloudWatch Event causing all future IAM roles created in an AWS account to be backdoored automatically. Pacu aims at being used against existing AWS infrastructure. </p> <p>Stratus Red Team is self-contained and does not necessitate prior infrastructure or configuration in your cloud environment. You can also use it programmatically, from Go code, as a library.</p>"},{"location":"comparison/#amazon-guardduty-tester-by-aws","title":"Amazon GuardDuty Tester by AWS","text":"<p>Credit: Description by Status Red Team</p> <p>Amazon GuardDuty Tester is helpful to trigger GuardDuty findings. However, it is tightly coupled with GuardDuty and is a product-specific tool, even within the AWS ecosystem. If GuardDuty doesn't detect an attack technique, you won't find it in here.</p>"},{"location":"comparison/#aws-cloudsaga-by-aws","title":"AWS CloudSaga by AWS","text":""},{"location":"comparison/#credit-description-by-status-red-team","title":"Credit: Description by Status Red Team","text":"<p>AWS CloudSaga has a few simulation scenarios (five at the time of writing). Some of them are more focused around identifying vulnerable resources in your account (such as <code>imds_reveal</code> listing your EC2 instances without IMDSv2 enforced), while others are designed to simulate attacker behavior.</p> <p>The attacker behavior implemented by AWS Cloud Saga emulates several stages of the attack lifecycle, while Stratus Red Team purposely attempts to stay as granular as possible (see: Philosophy). As much as possible, Stratus Red Team techniques also reference real-world incidents or breaches.</p> <p>Finally, AWS CloudSaga is by design specific to AWS, while Stratus Red Team supports AWS, Azure, GCP and even Kubernetes.</p>"},{"location":"comparison/#cloudgoat-by-rhino-security-labs","title":"CloudGoat by Rhino Security Labs","text":"<p>Credit: Description by Status Red Team</p> <p>CloudGoat is Rhino Security Labs' \"Vulnerable by Design\" AWS deployment tool. It allows you to hone your cloud cybersecurity skills by creating and completing several \"capture-the-flag\" style scenarios.</p> <p>CloudGoat is focused on spinning up vulnerable AWS infrastructure, so that you can exploit it to find a flag through a complete exploitation chain.</p> <p>Use CloudGoat to: practice your AWS offensive security and enumeration skills.</p> <p>Use Stratus Red Team to: emulate adversary behavior in AWS to validate your threat detection.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>We welcome pull requests, contributions and feedback! For any bug report or feedback, open an issue.</p>"},{"location":"contributing/#contributing-to-a-new-attack-technique","title":"Contributing to a new attack technique","text":"<p>When contributing a new public attack technique (modules under the <code>attack-technique</code> directory), please submit a pull request. Locate a sample module in the <code>attack-internal</code> directory for an example of module structure.</p>"},{"location":"contributing/#contributing-to-the-core-of-derf","title":"Contributing to the core of DeRF","text":"<p>When contributing to the core of DeRF (any code under the <code>/aws-proxy-app</code> or <code>derf-deployment</code> directory), please submit a pull request.</p>"},{"location":"faq/","title":"F.A.Q.","text":""},{"location":"faq/#what-permissions-do-i-need-to-run-derf","title":"What permissions do I need to run DeRF?","text":"<p>DeRF separates the permissions required to deploy the infrastructure from the permissions required to execute an attack.</p>"},{"location":"faq/#deployment-permissions","title":"Deployment Permissions","text":"<p>See here for AWS policy detailing the permissions required to in AWS and GCP to deploy the DeRF.</p>"},{"location":"faq/#end-user-execution-permissions","title":"End User Execution Permissions","text":"<p>See here for detailed instructions on the permissions required in GCP to execute attacks.</p>"},{"location":"faq/#how-does-the-derf-persist-state","title":"How does the DeRF persist state?","text":"<p>The DeRF uses a remote backend for its Terraform configurations and as such, a S3 bucket is required to initialize and deploy The DeRF.  See <code>./env-prod/TEMPLATE.conf</code>.   This is a requirement because AWS Access Keys are generated during DeRF deployment and the Access Key Secret value will persist in terraform state.  Its better these secrets persists in an encrypted S3 bucket with appropriate access controls rather than the operators local machine. </p>"},{"location":"faq/#how-can-i-add-my-own-attack-techniques-to-the-derf","title":"How can I add my own attack techniques to the DeRF?","text":"<p>Review the documentation at <code>docs/user-guide/aws-attack-creation.md</code> for instructions on creating your own attacks targeting AWS resources.  See sample attack modules under the <code>attacks-internal</code> directory.</p>"},{"location":"faq/#can-i-use-the-derf-to-execute-attack-techniques-against-my-own-infrastructure","title":"Can I use the DeRF to execute attack techniques against my own infrastructure?","text":"<ul> <li>AWS: This tool spins up DeRF specific resources in specified AWS account in order for the attack techniques to operate on.  Within each module you can see the required resources in the <code>infra.tf</code> file.  If you wanted the attack technique modules to target different infrastructure it would require some terraform surgery.  <ol> <li>Comment out the contents of the <code>infra.tf</code> file.</li> <li>Within the <code>local.tf</code> file, replace the values of the local variables with the hardcoded values from your BYO Infrastructure.</li> <li>Comment out the variables from the infrastructure no longer used in the <code>variables.tf</code> file.</li> </ol> </li> </ul>"},{"location":"faq/#how-do-i-destroy-the-derf-infrastructure","title":"How do I destroy the DeRF Infrastructure?","text":"<p>All deployed resources across both AWS and GCP will be removed with terraform. From the <code>env-prod/</code> directory: <pre><code>terraform destroy -var-file=derf.tfvars\n</code></pre></p>"},{"location":"Deployment/connect-to-github-repo/","title":"Connect To Github","text":""},{"location":"Deployment/connect-to-github-repo/#connecting-cloud-build-with-the-derf-github-repo","title":"Connecting Cloud Build with the DeRF Github Repo","text":"<p>The DeRF deployment creates a Continuous Deployment (CD) pipeline from the <code>DeRF</code> Github Repository.  Before deploying into a GCP Project for the first time, you will need to create the connection between your GCP Project and Github.</p> <p>To connect a GitHub repository to a host connection, complete the following steps:</p> <ol> <li>Connect to a repository on the Google Cloud console.</li> <li>With the link above, the side panel called 'Connect Repository' will be shown on the right. Select the Github Cloud Build Github App. </li> <li>On the next screen, connect to the <code>vectra-ai-research</code> github account and the <code>vectra-ai-research/derf</code> Repository.  Click <code>Connect</code> to create the connection. </li> <li>After the connection is made, on the Cloud Build Repositories page on Google Cloud console you will see the connection listed.  At anytime you can delete or disable the connection although this will prevent you from receiving updates to the <code>aws-proxy-app</code>. </li> </ol>"},{"location":"Deployment/connect-to-github-repo/#approving-builds","title":"Approving Builds","text":"<p>With this connection between Cloud Build and Github, whenever there is an update to the <code>aws-proxy-app</code> a Build will be queued in your GCP Project. The Build requires your manual approval in order to run and deploy a new version to Cloud Run.</p>"},{"location":"Deployment/deployment-permissions/","title":"Deployment Permissions","text":""},{"location":"Deployment/deployment-permissions/#deployment-permissions","title":"Deployment Permissions","text":""},{"location":"Deployment/deployment-permissions/#aws","title":"AWS","text":"<p>After attempting to document individually the AWS IAM permissions required to deploy and destroy The DeRF via terraform, I gave up when the list of permissions became extensive and were \"effective admin\".  I am recommending The DeRF be deployed to an targeted AWS Account with a user having the AdministratorAccess Managed Policy.</p>"},{"location":"Deployment/deployment-permissions/#gcp-deployment","title":"GCP Deployment","text":"<p>Below are the Google Managed Roles required to deploy the DeRF into a Google Project.  It does not take into account the permissions required to create the project in the first place.</p> <ul> <li>roles/secretmanager.admin applied at the Project-Level<ul> <li>Required to create Secrets used to store AWS Access Key Id and Secrets and assign Roles at the Secret-Level.</li> </ul> </li> <li>roles/cloudbuild.builds.editor applied at the Project-Level<ul> <li>Required to deploy a Cloud Build trigger used in the deployment pipeline of the Cloud Run <code>aws-proxy-app</code></li> </ul> </li> <li>roles/run.admin applied at the Project-Level<ul> <li>Required to deploy the Cloud Run <code>aws-proxy-app</code></li> </ul> </li> <li>roles/artifactregistry.admin applied at the Project-Level<ul> <li>Required when triggering Cloud Builds to store built image artifacts</li> </ul> </li> <li>roles/workflows.admin applied at the Project-Level<ul> <li>Required to deploy and invoke Google Workflows</li> </ul> </li> <li>roles/resourcemanager.projectIamAdmin applied at the Project-Level<ul> <li>Required to set IAM Policy at the Project level.</li> </ul> </li> </ul>"},{"location":"Deployment/derf-deployment/","title":"DeRF Deployment","text":""},{"location":"Deployment/derf-deployment/#deployment-steps","title":"Deployment Steps","text":"<ol> <li>Complete Prerequisites see below.</li> <li>Connect Cloud Build to Github. See instructions.</li> <li>Complete System Requirements see below.</li> <li>Clone the Github repo to your local system. <pre><code>git clone https://github.com/vectra-ai-research/derf.git\n</code></pre></li> <li>Deploy the DeRF via Terraform from the <code>./env-prod</code> directory. <pre><code>terraform init -backend-config=derf.conf\n</code></pre> <pre><code>terraform plan -var-file=derf.tfvars\n</code></pre> <pre><code>terraform apply -var-file=derf.tfvars\n</code></pre></li> </ol>"},{"location":"Deployment/derf-deployment/#prerequisites","title":"Prerequisites","text":"<ol> <li>Cloud Accounts:<ul> <li>One AWS Account: <ul> <li>This is your targeted AWS Account where attacks will run.</li> </ul> </li> <li>One GCP Project: <ul> <li>A Google Cloud Project which will house the DeRF <code>aws-proxy-app</code> , a CI/CD pipeline enabling updates to the <code>aws-proxy-app</code> and a collection of cloud workflows needed for attack technique execution. </li> <li>Manual Step: Connect Cloud Build with the <code>derf</code> github repository. See instructions</li> </ul> </li> </ul> </li> <li>Terraform Variables<ul> <li>Fill out the values the <code>TEMPLATE.tfvars</code> file located in <code>./env-prod</code> directory.</li> <li>Rename this file to be <code>derf.tfvars</code></li> <li>Terraform Variables:<ul> <li>aws_primary_id: The AWS Account Number of your targets AWS Account</li> <li>aws_primary_profile: The profile used to when authenticating to the targeted AWS account. Profile can be configured in the <code>~/.aws/config</code> file. More on configuring profiles</li> <li>region: The AWS region to execute attacks in.</li> <li>pathToAWSConfig: The absolute path to your <code>~/.aws/config</code> file. <ul> <li>Example: \"/Users/computer-name/.aws/config\"</li> </ul> </li> <li>gcp_deployment_project_id: The ID of the GCP Project to deploy the DeRF Framework components.  This value will likely consist of both letters and numbers but never numbers alone.</li> </ul> </li> </ul> </li> <li>Backend Configuration<ul> <li>Fill out the values the <code>TEMPLATE.conf</code> file located in <code>./env-prod</code> directory.</li> <li>Rename this file to be <code>derf.conf</code></li> <li>Why run Terraform with a remote backend?</li> <li>Running a remote backend to an encrypted S3 bucket is recommended as AWS Access Keys are generated during this <code>Terraform Apply</code> and will otherwise be retained locally in the state file.</li> <li>Backend configuration values:<ul> <li>region: The AWS region to execute attacks in.</li> <li>bucket: Name of the S3 bucket to store remote Terraform State. This should minimally be SSE-S3 encrypted.<ul> <li>Example: \"my-bucket-002984\"</li> </ul> </li> <li>profile: The profile used to when authenticating to the AWS Account where the S3 bucket resides. This profile needs write access to the S3 bucket. It does not need to be the same as the Targeted AWS Account. Profile can be configured in the <code>~/.aws/config</code> file. More on configuring profiles </li> </ul> </li> </ul> </li> </ol>"},{"location":"Deployment/derf-deployment/#system-requirements","title":"System Requirements","text":"Terraform Installation <p>The DeRF has only been tested with Terraform version 1.4.5.  In order to manage multiple versions of Terraform on your system, install <code>tfenv</code> command line tool, allowing you to switch between different versions of terraform. </p> <ol> <li>Install <code>tfenv</code> <pre><code>brew install tfenv\n</code></pre></li> <li>Install Terraform version 1.4.5 <pre><code>tfenv install 1.4.5\n</code></pre></li> <li>Set version 1.4.5 as your default version <pre><code>tfenv use 1.4.5\n</code></pre></li> </ol> gcloud Installation <p>During deployment, the <code>gcloud</code> cli is invoked to trigger Cloud Build and deploy the <code>aws-proxy-app</code> to Cloud Run. As a result, <code>gcloud</code>  will need to be installed on your local system in order to deploy The DeRF.  </p> <p>Download and install the <code>gcloud</code> cli per instructions located here.</p>"},{"location":"Deployment/derf-deployment/#troubleshooting","title":"Troubleshooting","text":"<p>If you already have Terraform on your system, you may need to unlink the cask with the following command before <code>tfenv</code> will take over Terraform installation. <pre><code>brew unlink terraform\n</code></pre></p>"},{"location":"attack-techniques/list/","title":"Supported Platforms","text":"<p>Currently, DeRF only comes with attack techniques for AWS.  See Getting Started for deployment instructions.</p>"},{"location":"attack-techniques/list/#list-of-all-attack-techniques","title":"List of all Attack Techniques","text":"<p>This page contains the list of all DeRF Attack Techniques.</p> Name Platform MITRE ATT&amp;CK Tactics Delete CloudTrail Trail AWS Defense Evasion Stop CloudTrail Logging AWS Defense Evasion Disable CloudTrail Logging Through Event Selectors AWS Defense Evasion CloudTrail Logs Impairment Through S3 Lifecycle Rule AWS Defense Evasion Attempt to Leave the AWS Organization AWS Defense Evasion AWS Remove VPC Flow Logs AWS Defense Evasion Exfiltrate EBS Snapshot by Sharing It AWS Exfiltration Download EC2 Instance User Data AWS Discovery Retrieve EC2 Password Data AWS Credential Access Steal EC2 Instance Credentials AWS Credential Access Retrieve and Decrypt SSM Parameters AWS Credential Access AWS Retrieve a High Number of Secrets Manager secrets AWS Credential Access Execute Commands on EC2 Instance via User Data AWS Execution Impersonate GCP Service Accounts GCP Privilege Escalation"},{"location":"attack-techniques/aws/cloudtrail-delete/","title":"Delete CloudTrail Trail","text":"<p>Platform: AWS</p>"},{"location":"attack-techniques/aws/cloudtrail-delete/#mitre-attck-tactics","title":"MITRE ATT&amp;CK Tactics","text":"<ul> <li>Defense Evasion</li> </ul>"},{"location":"attack-techniques/aws/cloudtrail-delete/#description","title":"Description","text":"<p>Delete a CloudTrail trail simulating an attacker disrupting logging to evade detection.</p>"},{"location":"attack-techniques/aws/cloudtrail-delete/#attacker-actions","title":"Attacker Actions:","text":"<ul> <li>Deletes a CloudTrail trail.</li> <li>Resulting event name: <code>DeleteTrail</code></li> <li>Assigned IAM Permission: <code>cloudtrail:DeleteTrail</code></li> </ul>"},{"location":"attack-techniques/aws/cloudtrail-delete/#workflow-inputs","title":"Workflow Inputs:","text":"<p>Specify which user this attack should run as.  <pre><code>{\"user\":\"user01\"}\n{\"user\":\"user02\"}\n</code></pre></p>"},{"location":"attack-techniques/aws/cloudtrail-delete/#clean-up","title":"Clean Up:","text":"<ul> <li>Recreates the CloudTrail trail.</li> <li>Executed as the <code>DeRF Default User</code></li> </ul>"},{"location":"attack-techniques/aws/cloudtrail-delete/#execution-instructions","title":"Execution Instructions","text":"<ul> <li>See User Guide for Execution Instructions via the Google Cloud Console</li> <li>Programmatically execute this workflow with the following cli command:</li> </ul> <pre><code>gcloud workflows run aws-delete-cloudtrail-trail `--data={\"user\": \"user01\"}` \n</code></pre>"},{"location":"attack-techniques/aws/cloudtrail-delete/#detection-artifacts","title":"Detection Artifacts","text":"<p>Identify when a CloudTrail trail is deleted, through CloudTrail's <code>DeleteTrail</code> event.</p> <p>GuardDuty also provides a dedicated finding type, Stealth:IAMUser/CloudTrailLoggingDisabled.</p>"},{"location":"attack-techniques/aws/cloudtrail-event-selectors/","title":"Disable CloudTrail Logging Through Event Selectors","text":"<p>Platform: AWS</p>"},{"location":"attack-techniques/aws/cloudtrail-event-selectors/#mitre-attck-tactics","title":"MITRE ATT&amp;CK Tactics","text":"<ul> <li>Defense Evasion</li> </ul>"},{"location":"attack-techniques/aws/cloudtrail-event-selectors/#description","title":"Description","text":"<p>Disrupt CloudTrail Logging by creating an event selector on the Trail, filtering out all management events.</p>"},{"location":"attack-techniques/aws/cloudtrail-event-selectors/#attacker-actions","title":"Attacker Actions:","text":"<ul> <li>Updates the in-scope events captured by a Cloudtrail to exclude all management-plane events.</li> <li>Resulting event name: <code>PutEventSelector</code></li> <li>Assigned IAM Permission: <code>cloudtrail:PutEventSelector</code></li> </ul>"},{"location":"attack-techniques/aws/cloudtrail-event-selectors/#workflow-inputs","title":"Workflow Inputs:","text":"<p>Specify which user this attack should run as.  <pre><code>{\"user\":\"user01\"}\n{\"user\":\"user02\"}\n</code></pre></p>"},{"location":"attack-techniques/aws/cloudtrail-event-selectors/#clean-up","title":"Clean Up:","text":"<ul> <li>Reverts the event selectors on Cloudtrail trail, resuming logging of management-plane events</li> <li>Executed as the <code>DeRF Default User</code></li> </ul>"},{"location":"attack-techniques/aws/cloudtrail-event-selectors/#execution-instructions","title":"Execution Instructions","text":"<ul> <li>See User Guide for Execution Instructions via the Google Cloud Console</li> <li>Programmatically execute this workflow with the following cli command:</li> </ul> <pre><code>gcloud workflows run aws-cloudtrail-event-selector-srt `--data={\"user\": \"user01\"}` \n</code></pre>"},{"location":"attack-techniques/aws/cloudtrail-event-selectors/#detection-artifacts","title":"Detection Artifacts","text":"<p>Identify when the scope of CloudTrail logging is narrowed through CloudTrail's <code>PutEventSelectors</code> event.</p>"},{"location":"attack-techniques/aws/cloudtrail-lifecycle-rules/","title":"CloudTrail Logs Impairment Through S3 Lifecycle Rule","text":"<p>Platform: AWS</p>"},{"location":"attack-techniques/aws/cloudtrail-lifecycle-rules/#mitre-attck-tactics","title":"MITRE ATT&amp;CK Tactics","text":"<ul> <li>Defense Evasion</li> </ul>"},{"location":"attack-techniques/aws/cloudtrail-lifecycle-rules/#description","title":"Description","text":"<p>Set a 1-day retention policy on the S3 bucket used by a CloudTrail Trail, using a S3 Lifecycle Rule.</p>"},{"location":"attack-techniques/aws/cloudtrail-lifecycle-rules/#attacker-actions","title":"Attacker Actions:","text":"<ul> <li>Updates the Lifecycle rule on the S3 bucket backing the CloudTrail trail to 1 day.</li> <li>Resulting event name: <code>PutBucketLifecycle</code></li> <li>Assigned IAM Permission: <code>s3:PutBucketLifecycle</code></li> </ul>"},{"location":"attack-techniques/aws/cloudtrail-lifecycle-rules/#workflow-inputs","title":"Workflow Inputs:","text":"<p>Specify which user this attack should run as.  <pre><code>{\"user\":\"user01\"}\n{\"user\":\"user02\"}\n</code></pre></p>"},{"location":"attack-techniques/aws/cloudtrail-lifecycle-rules/#clean-up","title":"Clean Up:","text":"<ul> <li>Resets Lifecycle configuration to 30 days.</li> <li>Executed as the <code>DeRF Default User</code></li> </ul>"},{"location":"attack-techniques/aws/cloudtrail-lifecycle-rules/#execution-instructions","title":"Execution Instructions","text":"<ul> <li>See User Guide for Execution Instructions via the Google Cloud Console</li> <li>Programmatically execute this workflow with the following cli command:</li> </ul> <pre><code>gcloud workflows run aws-delete-cloudtrail-trail `--data={\"user\": \"user01\"}` \n</code></pre>"},{"location":"attack-techniques/aws/cloudtrail-lifecycle-rules/#detection-artifacts","title":"Detection Artifacts","text":"<p>Identify when lifecycle rule with a short expiration is applied to an S3 bucket used for CloudTrail logging.</p> <p>The CloudTrail event <code>PutBucketLifecycle</code> and its attribute requestParameters.LifecycleConfiguration.Rule.Expiration.Days can be used.</p>"},{"location":"attack-techniques/aws/cloudtrail-stop/","title":"Stop CloudTrail Trail","text":"<p>Platform: AWS</p>"},{"location":"attack-techniques/aws/cloudtrail-stop/#mitre-attck-tactics","title":"MITRE ATT&amp;CK Tactics","text":"<ul> <li>Defense Evasion</li> </ul>"},{"location":"attack-techniques/aws/cloudtrail-stop/#description","title":"Description","text":"<p>Stop the recording of events from a CloudTrail trail simulating an attacker disrupting logging to evade detection.</p>"},{"location":"attack-techniques/aws/cloudtrail-stop/#attacker-actions","title":"Attacker Actions:","text":"<ul> <li>Stop a CloudTrail trail.</li> <li>Resulting event name: <code>StopTrail</code></li> <li>Assigned IAM Permission: <code>cloudtrail:StopTrail</code></li> </ul>"},{"location":"attack-techniques/aws/cloudtrail-stop/#workflow-inputs","title":"Workflow Inputs:","text":"<p>Specify which user this attack should run as.  <pre><code>{\"user\":\"user01\"}\n{\"user\":\"user02\"}\n</code></pre></p>"},{"location":"attack-techniques/aws/cloudtrail-stop/#clean-up","title":"Clean Up:","text":"<ul> <li>Restarts the CloudTrail trail.</li> <li>Executed as the <code>DeRF Default User</code></li> </ul>"},{"location":"attack-techniques/aws/cloudtrail-stop/#execution-instructions","title":"Execution Instructions","text":"<ul> <li>See User Guide for Execution Instructions via the Google Cloud Console</li> <li>Programmatically execute this workflow with the following cli command:</li> </ul> <pre><code>gcloud workflows run aws-delete-cloudtrail-trail `--data={\"user\": \"user01\"}` \n</code></pre>"},{"location":"attack-techniques/aws/cloudtrail-stop/#detection-artifacts","title":"Detection Artifacts","text":"<p>Identify when a CloudTrail trail is disabled, through CloudTrail's <code>StopLogging </code>event.</p> <p>GuardDuty  provides a dedicated finding type, Stealth:IAMUser/CloudTrailLoggingDisabled</p>"},{"location":"attack-techniques/aws/ec2-get-password-data/","title":"AWS Retrieve EC2 Password Data","text":"<p>Platform: AWS</p>"},{"location":"attack-techniques/aws/ec2-get-password-data/#mitre-attck-tactics","title":"MITRE ATT&amp;CK Tactics","text":"<ul> <li>Credential Access</li> </ul>"},{"location":"attack-techniques/aws/ec2-get-password-data/#description","title":"Description","text":"<p>Runs ec2:GetPasswordData (30) times over (30) different fictitious EC2 instances. This simulates an attacker attempting to retrieve RDP passwords on a high number of Windows EC2 instances.</p> <p>See https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_GetPasswordData.html</p>"},{"location":"attack-techniques/aws/ec2-get-password-data/#attacker-actions","title":"Attacker Actions:","text":"<ul> <li>Attempts to receive RDP password from fictitious EC2 Instance Id.</li> <li>Resulting event name: <code>GetPasswordData</code></li> <li>Assigned IAM Permission: <code>ec2:GetPasswordData</code></li> </ul>"},{"location":"attack-techniques/aws/ec2-get-password-data/#workflow-inputs","title":"Workflow Inputs:","text":"<p>Specify which user this attack should run as.  <pre><code>{\"user\":\"user01\"}\n{\"user\":\"user02\"}\n</code></pre></p>"},{"location":"attack-techniques/aws/ec2-get-password-data/#clean-up","title":"Clean Up:","text":"<p>None</p>"},{"location":"attack-techniques/aws/ec2-get-password-data/#execution-instructions","title":"Execution Instructions","text":"<ul> <li>See User Guide for Execution Instructions via the Google Cloud Console</li> <li>Programmatically execute this workflow with the following cli command:</li> </ul> <pre><code>gcloud workflows run aws-delete-cloudtrail-trail `--data={\"user\": \"user01\"}` \n</code></pre>"},{"location":"attack-techniques/aws/ec2-get-password-data/#detection-artifacts","title":"Detection Artifacts","text":"<p>Identify principals making a large number of ec2:GetPasswordData calls, using CloudTrail's GetPasswordData event.</p> <p>e</p>"},{"location":"attack-techniques/aws/ec2-get-user-data/","title":"EC2 Get User Data","text":"<p>Platform: AWS</p>"},{"location":"attack-techniques/aws/ec2-get-user-data/#mitre-attck-tactics","title":"MITRE ATT&amp;CK Tactics","text":"<ul> <li>Discovery</li> </ul>"},{"location":"attack-techniques/aws/ec2-get-user-data/#description","title":"Description","text":"<p>This simulates an attacker attempting to retrieve EC2 Instance User Data that frequently includes installation scripts and hard-coded secrets for deployment. This module results in an <code>Access Denied</code> error as the users are not granted the appropriate permissions</p>"},{"location":"attack-techniques/aws/ec2-get-user-data/#attacker-actions","title":"Attacker Actions:","text":"<ul> <li>Calls the <code>DescribeInstanceAttribute</code> API specifying the <code>userData</code> attribute (3) times on a fictitious EC2 Instance.</li> <li>Resulting event name: <code>DescribeInstanceAttribute</code></li> <li>Assigned IAM Permission: None</li> </ul>"},{"location":"attack-techniques/aws/ec2-get-user-data/#workflow-inputs","title":"Workflow Inputs:","text":"<p>Specify which user this attack should run as.  <pre><code>{\"user\":\"user01\"}\n{\"user\":\"user02\"}\n</code></pre></p>"},{"location":"attack-techniques/aws/ec2-get-user-data/#clean-up","title":"Clean Up:","text":"<p>None - no resources are modified.</p>"},{"location":"attack-techniques/aws/ec2-get-user-data/#execution-instructions","title":"Execution Instructions","text":"<ul> <li>See User Guide for Execution Instructions via the Google Cloud Console</li> <li>Programmatically execute this workflow with the following cli command:</li> </ul> <pre><code>gcloud workflows run aws-ec2-get-user-data `--data={\"user\": \"user01\"}` \n</code></pre>"},{"location":"attack-techniques/aws/ec2-get-user-data/#detection-artifacts","title":"Detection Artifacts","text":"<p>Identify when a CloudTrail trail is deleted, through CloudTrail's <code>DescribeInstanceAttribute</code> event.</p>"},{"location":"attack-techniques/aws/ec2-modify-user-data/","title":"Execute Commands on EC2 Instance via User Data","text":"<p>Platform: AWS</p>"},{"location":"attack-techniques/aws/ec2-modify-user-data/#mitre-attck-tactics","title":"MITRE ATT&amp;CK Tactics","text":"<ul> <li>Execution</li> </ul>"},{"location":"attack-techniques/aws/ec2-modify-user-data/#description","title":"Description","text":"<p>Executes code on an EC2 instance as root through User Data.</p> <p>References:</p> <ul> <li>https://hackingthe.cloud/aws/exploitation/local-priv-esc-mod-instance-att/</li> <li>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html</li> </ul>"},{"location":"attack-techniques/aws/ec2-modify-user-data/#attacker-actions","title":"Attacker Actions:","text":"<ul> <li>Attacker stops the EC2 instance before modifying User Data.</li> <li>Resulting event name: <code>StopInstances</code></li> <li>Assigned IAM Permission: <code>ec2:StopInstances</code></li> <li>Attacker maliciously updates the User Data to be executed on the VM.</li> <li>Resulting event name: <code>ModifyInstanceAttribute</code></li> <li>Assigned IAM Permission: <code>ec2:ModifyInstanceAttribute</code></li> <li>Attacker restarts the EC2 instance in order for the code to execute on the machine.</li> <li>Resulting event name: <code>StartInstances</code></li> <li>Assigned IAM Permission: <code>ec2:StartInstances</code></li> </ul>"},{"location":"attack-techniques/aws/ec2-modify-user-data/#workflow-inputs","title":"Workflow Inputs:","text":"<p>Specify which user this attack should run as.  <pre><code>{\"user\":\"user01\"}\n{\"user\":\"user02\"}\n</code></pre></p>"},{"location":"attack-techniques/aws/ec2-modify-user-data/#clean-up","title":"Clean Up:","text":"<p>None - no resources are modified.</p>"},{"location":"attack-techniques/aws/ec2-modify-user-data/#execution-instructions","title":"Execution Instructions","text":"<ul> <li>See User Guide for Execution Instructions via the Google Cloud Console</li> <li>Programmatically execute this workflow with the following cli command:</li> </ul> <pre><code>gcloud workflows run aws-ec2-get-user-data `--data={\"user\": \"user01\"}` \n</code></pre>"},{"location":"attack-techniques/aws/ec2-modify-user-data/#detection-artifacts","title":"Detection Artifacts","text":"<p>Identify when the <code>ModifyInstanceAttribute</code> event occurs with requestParameters.userData non-empty and containing suspicious or unexpected data. It's generally not expected that the user data of an EC2 instance changes often, especially with the popularity of immutable machine images, provisioned before instantiation.</p>"},{"location":"attack-techniques/aws/ec2-share-ebs-snapshot/","title":"EC2 Shared EBS Snapshot","text":"<p>Platform: AWS</p>"},{"location":"attack-techniques/aws/ec2-share-ebs-snapshot/#mitre-attck-tactics","title":"MITRE ATT&amp;CK Tactics","text":"<ul> <li>Exfiltration</li> </ul>"},{"location":"attack-techniques/aws/ec2-share-ebs-snapshot/#description","title":"Description","text":"<p>This attack shares an EBS Snapshot with an external, fictitious AWS account, (012345678912)</p>"},{"location":"attack-techniques/aws/ec2-share-ebs-snapshot/#attacker-actions","title":"Attacker Actions:","text":"<ul> <li>Updated the attributes of an EBS Snapshot to an external, fictitious AWS account.</li> <li>Resulting event name: <code>ModifySnapshotAttribute</code></li> <li>Assigned IAM Permission: <code>ec2:ModifySnapshotAttribute</code></li> </ul>"},{"location":"attack-techniques/aws/ec2-share-ebs-snapshot/#workflow-inputs","title":"Workflow Inputs:","text":"<p>Specify which user this attack should run as.  <pre><code>{\"user\":\"user01\"}\n{\"user\":\"user02\"}\n</code></pre></p>"},{"location":"attack-techniques/aws/ec2-share-ebs-snapshot/#clean-up","title":"Clean Up:","text":"<ul> <li>Recreates the CloudTrail trail.</li> <li>Executed as the <code>DeRF Default User</code></li> </ul>"},{"location":"attack-techniques/aws/ec2-share-ebs-snapshot/#execution-instructions","title":"Execution Instructions","text":"<ul> <li>See User Guide for Execution Instructions via the Google Cloud Console</li> <li>Programmatically execute this workflow with the following cli command:</li> </ul> <pre><code>gcloud workflows run aws-delete-cloudtrail-trail `--data={\"user\": \"user01\"}` \n</code></pre>"},{"location":"attack-techniques/aws/ec2-share-ebs-snapshot/#detection-artifacts","title":"Detection Artifacts","text":"<p>Identify when a CloudTrail trail is deleted, through CloudTrail's <code>DeleteTrail</code> event.</p> <p>GuardDuty also provides a dedicated finding type, Stealth:IAMUser/CloudTrailLoggingDisabled.</p>"},{"location":"attack-techniques/aws/ec2-steal-instance-credentials/","title":"AWS Steal EC2 Instance Credentials","text":"<p>Platform: AWS</p>"},{"location":"attack-techniques/aws/ec2-steal-instance-credentials/#mitre-attck-tactics","title":"MITRE ATT&amp;CK Tactics","text":"<ul> <li>Credential Access</li> </ul>"},{"location":"attack-techniques/aws/ec2-steal-instance-credentials/#description","title":"Description","text":"<p>Simulates the theft of EC2 instance credentials from the Instance Metadata Service and the use of the stolen credentials outside AWS IP space.</p>"},{"location":"attack-techniques/aws/ec2-steal-instance-credentials/#attacker-actions","title":"Attacker Actions:","text":"<ul> <li>Attempts to SSM into EC2 instance with defined user.  Once credentials are retrieved, the workflow then calls the API 'DescribeInstances' with the EC2 instance profile credentials from the proxy-app in Google CLoud (outside AWS IP space).</li> <li>Resulting event name: <code>DescribeInstances</code></li> <li>Assigned IAM Permission: <code>ec2:DescribeInstances</code></li> </ul>"},{"location":"attack-techniques/aws/ec2-steal-instance-credentials/#workflow-inputs","title":"Workflow Inputs:","text":"<p>Specify which user this attack should run as.  <pre><code>{\"user\":\"user01\"}\n{\"user\":\"user02\"}\n</code></pre></p>"},{"location":"attack-techniques/aws/ec2-steal-instance-credentials/#clean-up","title":"Clean Up:","text":"<p>None</p>"},{"location":"attack-techniques/aws/ec2-steal-instance-credentials/#execution-instructions","title":"Execution Instructions","text":"<ul> <li>See User Guide for Execution Instructions via the Google Cloud Console</li> <li>Programmatically execute this workflow with the following cli command:</li> </ul> <pre><code>gcloud workflows run aws-delete-cloudtrail-trail `--data={\"user\": \"user01\"}` \n</code></pre>"},{"location":"attack-techniques/aws/ec2-steal-instance-credentials/#detection-artifacts","title":"Detection Artifacts","text":"<p>GuardDuty provides two findings to identify stolen EC2 instance credentials.</p> <ul> <li> identifies EC2 instance credentials used from outside an AWS account. </li> <li> identifies EC2 instance credentials used from a different AWS account than the one of the EC2 instance.</li> </ul> <p>See also: .</p>"},{"location":"attack-techniques/aws/organizations-leave/","title":"Attempt to Leave the AWS Organization","text":"<p>Platform: AWS</p>"},{"location":"attack-techniques/aws/organizations-leave/#mitre-attck-tactics","title":"MITRE ATT&amp;CK Tactics","text":"<ul> <li>Defense Evasion</li> </ul>"},{"location":"attack-techniques/aws/organizations-leave/#description","title":"Description","text":"<p>Attempts to leave the AWS Organization.  Since the execution users are not granted this permission, the attempt will fail and result in an AccessDenied error.</p>"},{"location":"attack-techniques/aws/organizations-leave/#attacker-actions","title":"Attacker Actions:","text":"<ul> <li>Attempts to leave an AWS Organization.</li> <li>Resulting event name: <code>LeaveOrganization</code></li> <li>Assigned IAM Permission: <code>organizations:LeaveOrganization</code></li> </ul>"},{"location":"attack-techniques/aws/organizations-leave/#workflow-inputs","title":"Workflow Inputs:","text":"<p>Specify which user this attack should run as.  <pre><code>{\"user\":\"user01\"}\n{\"user\":\"user02\"}\n</code></pre></p>"},{"location":"attack-techniques/aws/organizations-leave/#clean-up","title":"Clean Up:","text":"<ul> <li>Recreates the CloudTrail trail.</li> <li>Executed as the <code>DeRF Default User</code></li> </ul>"},{"location":"attack-techniques/aws/organizations-leave/#execution-instructions","title":"Execution Instructions","text":"<ul> <li>See User Guide for Execution Instructions via the Google Cloud Console</li> <li>Programmatically execute this workflow with the following cli command:</li> </ul> <pre><code>gcloud workflows run aws-delete-cloudtrail-trail `--data={\"user\": \"user01\"}` \n</code></pre>"},{"location":"attack-techniques/aws/organizations-leave/#detection-artifacts","title":"Detection Artifacts","text":"<p>Any attempts from a child account to leave its AWS Organization should be considered suspicious as leaving the organization can remove security controls enforced from the Organizational Management Account</p>"},{"location":"attack-techniques/aws/secretsmanager-retrieve-secrets/","title":"AWS Retrieve a High Number of Secrets Manager secrets","text":"<p>Platform: AWS</p>"},{"location":"attack-techniques/aws/secretsmanager-retrieve-secrets/#mitre-attck-tactics","title":"MITRE ATT&amp;CK Tactics","text":"<ul> <li>Credential Access</li> </ul>"},{"location":"attack-techniques/aws/secretsmanager-retrieve-secrets/#description","title":"Description","text":"<p>Lists the secrets stored in Secrets Manager and retrieves (20) secret values</p>"},{"location":"attack-techniques/aws/secretsmanager-retrieve-secrets/#attacker-actions","title":"Attacker Actions:","text":"<ul> <li>First, lists the secrets stored in Secrets Manager in the current region.</li> <li>Secondly, retrieves the values of (20) secrets stored in Secrets Manager</li> <li>Resulting event names: <ul> <li><code>ListSecrets</code></li> <li><code>GetSecretValue</code></li> </ul> </li> <li>Assigned IAM Permission: <ul> <li><code>secretsmanager:ListSecrets</code></li> <li><code>secretsmanager:GetSecretValue</code></li> </ul> </li> </ul>"},{"location":"attack-techniques/aws/secretsmanager-retrieve-secrets/#workflow-inputs","title":"Workflow Inputs:","text":"<p>Specify which user this attack should run as.  <pre><code>{\"user\":\"user01\"}\n{\"user\":\"user02\"}\n</code></pre></p>"},{"location":"attack-techniques/aws/secretsmanager-retrieve-secrets/#clean-up","title":"Clean Up:","text":"<p>None</p>"},{"location":"attack-techniques/aws/secretsmanager-retrieve-secrets/#execution-instructions","title":"Execution Instructions","text":"<ul> <li>See User Guide for Execution Instructions via the Google Cloud Console</li> <li>Programmatically execute this workflow with the following cli command:</li> </ul> <pre><code>gcloud workflows run aws-delete-cloudtrail-trail `--data={\"user\": \"user01\"}` \n</code></pre>"},{"location":"attack-techniques/aws/secretsmanager-retrieve-secrets/#detection-artifacts","title":"Detection Artifacts","text":"<p>Identify principals retrieving a high number of secrets, through CloudTrail's GetSecretValue event.</p>"},{"location":"attack-techniques/aws/ssm-retrieve-securestring-parameters/","title":"AWS Retrieve and Decrypt SSM Parameters","text":"<p>Platform: AWS</p>"},{"location":"attack-techniques/aws/ssm-retrieve-securestring-parameters/#mitre-attck-tactics","title":"MITRE ATT&amp;CK Tactics","text":"<ul> <li>Credential Access</li> </ul>"},{"location":"attack-techniques/aws/ssm-retrieve-securestring-parameters/#description","title":"Description","text":"<p>Describes the SSM Parameters in an Account and retrieves and decrypts (30) SSM Parameters available in an AWS region.</p>"},{"location":"attack-techniques/aws/ssm-retrieve-securestring-parameters/#attacker-actions","title":"Attacker Actions:","text":"<ul> <li>First, lists the SSM Parameters in the current region.</li> <li>Secondly, retrieves the values of (30) SSM Parameters.</li> <li>Resulting event names: <ul> <li><code>DescribeParameters</code></li> <li><code>GetParameter</code></li> </ul> </li> <li>Assigned IAM Permission: <ul> <li><code>ssm:DescribeParameters</code></li> <li><code>ssm:GetParameter</code></li> </ul> </li> </ul>"},{"location":"attack-techniques/aws/ssm-retrieve-securestring-parameters/#workflow-inputs","title":"Workflow Inputs:","text":"<p>Specify which user this attack should run as.  <pre><code>{\"user\":\"user01\"}\n{\"user\":\"user02\"}\n</code></pre></p>"},{"location":"attack-techniques/aws/ssm-retrieve-securestring-parameters/#clean-up","title":"Clean Up:","text":"<p>None</p>"},{"location":"attack-techniques/aws/ssm-retrieve-securestring-parameters/#execution-instructions","title":"Execution Instructions","text":"<ul> <li>See User Guide for Execution Instructions via the Google Cloud Console</li> <li>Programmatically execute this workflow with the following cli command:</li> </ul> <pre><code>gcloud workflows run aws-delete-cloudtrail-trail `--data={\"user\": \"user01\"}` \n</code></pre>"},{"location":"attack-techniques/aws/ssm-retrieve-securestring-parameters/#detection-artifacts","title":"Detection Artifacts","text":"<p>Identify principals retrieving a high number of SSM Parameters, through CloudTrail's GetParameter event. It is especially suspicious when the <code>requestParameters</code> contains the following key value/pair <code>\"withDecryption\": true</code> as this might indicate the parameters retrieved were deemed sensitive enough to be KMS encrypted.</p>"},{"location":"attack-techniques/aws/vpc-remove-flow-log/","title":"AWS Remove VPC Flow Logs","text":"<p>Platform: AWS</p>"},{"location":"attack-techniques/aws/vpc-remove-flow-log/#mitre-attck-tactics","title":"MITRE ATT&amp;CK Tactics","text":"<ul> <li>Defense Evasion</li> </ul>"},{"location":"attack-techniques/aws/vpc-remove-flow-log/#description","title":"Description","text":"<p>Removes a VPC Flog Logs configuration from a VPC.</p>"},{"location":"attack-techniques/aws/vpc-remove-flow-log/#attacker-actions","title":"Attacker Actions:","text":"<ul> <li>Deletes a VPC Flow Log from a VPC.</li> <li>Resulting event name: <code>DeleteFlowLogs</code></li> <li>Assigned IAM Permission: <code>ec2:DeleteFlowLogs</code></li> </ul>"},{"location":"attack-techniques/aws/vpc-remove-flow-log/#workflow-inputs","title":"Workflow Inputs:","text":"<p>Specify which user this attack should run as.  <pre><code>{\"user\":\"user01\"}\n{\"user\":\"user02\"}\n</code></pre></p>"},{"location":"attack-techniques/aws/vpc-remove-flow-log/#clean-up","title":"Clean Up:","text":"<ul> <li>Recreates the VPC Flow Log configuration.</li> <li>Executed as the <code>DeRF Default User</code></li> </ul>"},{"location":"attack-techniques/aws/vpc-remove-flow-log/#execution-instructions","title":"Execution Instructions","text":"<ul> <li>See User Guide for Execution Instructions via the Google Cloud Console</li> <li>Programmatically execute this workflow with the following cli command:</li> </ul> <pre><code>gcloud workflows run aws-delete-cloudtrail-trail `--data={\"user\": \"user01\"}` \n</code></pre>"},{"location":"attack-techniques/aws/vpc-remove-flow-log/#detection-artifacts","title":"Detection Artifacts","text":"<p>Use the Cloudtrail event <code>DeleteFlowLogs</code> to identify activity.</p>"},{"location":"attack-techniques/gcp/impersonate-service-accounts/","title":"Delete CloudTrail Trail","text":"<p>Platform: AWS</p>"},{"location":"attack-techniques/gcp/impersonate-service-accounts/#mitre-attck-tactics","title":"MITRE ATT&amp;CK Tactics","text":"<ul> <li>Privilege Escalation</li> </ul>"},{"location":"attack-techniques/gcp/impersonate-service-accounts/#description","title":"Description","text":"<p>Attempts to impersonate 10 different GCP service accounts in the project. Service account impersonation in GCP is the retrieval temporary credentials (OAuth bearer tokens) allowing the impersonator to 'act as' the targeted service account.</p>"},{"location":"attack-techniques/gcp/impersonate-service-accounts/#attacker-actions","title":"Attacker Actions:","text":"<ul> <li>Attempt to impersonate each of the 10 service accounts created for this detection. Only one impersonation request will succeed, simulating a successful privilege escalation.</li> <li>Resulting event name: <code>GenerateAccessToken</code></li> <li>Assigned IAM Permission: <code>iam.serviceAccounts.actAs</code></li> </ul>"},{"location":"attack-techniques/gcp/impersonate-service-accounts/#workflow-inputs","title":"Workflow Inputs:","text":"<p>Specify which derf attacker service account this attack should run as.  <pre><code>{\"sa\":\"01\"}\n{\"sa\":\"02\"}\n</code></pre></p>"},{"location":"attack-techniques/gcp/impersonate-service-accounts/#clean-up","title":"Clean Up:","text":"<p>None</p>"},{"location":"attack-techniques/gcp/impersonate-service-accounts/#execution-instructions","title":"Execution Instructions","text":"<ul> <li>See User Guide for Execution Instructions via the Google Cloud Console</li> <li>Programmatically execute this workflow with the following cli command:</li> </ul> <pre><code>gcloud workflows run gcp-impersonate-sa-srt `--data={\"sa\": \"01\"}` \n</code></pre>"},{"location":"attack-techniques/gcp/impersonate-service-accounts/#detection-artifacts","title":"Detection Artifacts","text":"<p>Using GCP Admin Activity audit logs event <code>GenerateAccessToken</code>. This event is not included in default logging and needs to be enabled. Specifically, IAM data access activity logs need to be enabled.  The principal caller is recorded in the log whether the event was a success or resulted in an error.</p> <p></p>"},{"location":"user-guide/attack-execution-access-control/","title":"End User Execution Permissions","text":""},{"location":"user-guide/attack-execution-access-control/#attack-execution-access-control","title":"Attack Execution - Access Control","text":"<p>The ability to execute an attack corresponds to the ability to invoke Cloud Workflows in your DeRF GCP Project.</p>"},{"location":"user-guide/attack-execution-access-control/#roles","title":"Roles","text":""},{"location":"user-guide/attack-execution-access-control/#execute-attacks-only","title":"Execute Attacks Only","text":"<ul> <li>Only the Workflows Invoker Role is needed to invoke the workflows and subsequently execute an attack.<ul> <li>roles/workflows.invoker</li> </ul> </li> </ul>"},{"location":"user-guide/attack-execution-access-control/#execute-an-attack-and-triage-issues","title":"Execute an Attack AND Triage issues","text":"<ul> <li>Only the Workflows Invoker Role is needed to invoke the workflows and subsequently execute an attack.<ul> <li>roles/workflows.invoker</li> </ul> </li> <li>Additional ReadOnly Roles required to give visibility into the underlying infrastructure, view logs, etc<ul> <li>roles/run.viewer</li> <li>roles/cloudbuild.builds.viewer</li> <li>roles/logging.viewer</li> </ul> </li> </ul>"},{"location":"user-guide/attack-execution-access-control/#best-practices-for-role-assignment-in-gcp","title":"Best Practices for Role Assignment in GCP","text":"<p>Its best practice to assign the above clusters of Roles to groups rather than individual users or service accounts.     - If Google Workspace is your primary Identity Provider, create a group and assign membership under 'Directory -&gt; Groups'. Once created in Google Workspace, your groups for Attack Execution will be available to assign Roles.     - If federating Google Workspace against another Identity Provider, create a group and assign membership in your Identity Provider. Sync the group and its members from your Identity Provider to Google Workspace with automatic SCIM provisioning.  Once populated in Google Workspace, your groups for Attack Execution will be available to assign Roles.     - If using Cloud Identity, from the cloud console, navigate to the Groups page.  Create a group and assign membership.  Once created, your groups for Attack Execution will be available to assign Roles.</p>"},{"location":"user-guide/aws-attack-creation/","title":"AWS Attack Technique Creation","text":""},{"location":"user-guide/aws-attack-creation/#building-your-own-aws-attack","title":"Building your own AWS Attack","text":"<p>Its possible as your use cases grow, you will want to expand on the library of built-in attack techniques and create your own custom modules.  Follow this guide for working with The DeRF to create your own workflows that execute as AWS Attack Techniques</p> <ol> <li> <p>Fork the <code>derf</code> repo found here.</p> </li> <li> <p>From the top-level directory, <code>attacks-internal</code>, review the <code>sample-attack</code> directory for an example of the structure of an AWS attack module.  Every attack module should be a folder containing at least the following files:   </p> <ul> <li><code>attack.tf</code>: The Google Workflow, defined in terraform, which outlines the API calls to make in the attack sequence</li> <li><code>iam-permissions.tf</code>: Any additional permissions need for the DeRF Execution Users to perform the attack.  Refer to the <code>sample-attack</code> directory for an example of the resources to create.</li> <li><code>variable.tf</code>: Refer to the <code>sample-attack</code> directory for the common variable imported into every attack.</li> <li><code>infra.tf</code>: If the attack technique requires any new target infrastructure, define it in this file.</li> </ul> </li> <li> <p>Create your new custom DeRF attack technique as a new folder in the <code>attacks-internal</code> directory.</p> </li> <li> <p>From the top-level directory, <code>env-prod</code>, review the <code>aws-attack-techniques-internal.tf</code> file.  From this file, source the newly created attack module in the style of the <code>sample-attack-module</code>.</p> </li> <li> <p>Re-deploy the infrastructure  following instructions here including the re-initalizing of terraform.</p> </li> </ol>"},{"location":"user-guide/aws-attack-creation/#specifying-details-of-an-aws-attack-technique","title":"Specifying Details of an AWS Attack Technique","text":"<p>Details of every API call to AWS is specified in the Google Workflows in the <code>http.post</code> request and passed to the <code>aws-proxy-app</code> for processing.  Below is a detailed accounting of the variables which can be sent to the <code>aws-proxy-app</code> to detail the API call to AWS.</p>"},{"location":"user-guide/aws-attack-creation/#variables","title":"Variables","text":"<ul> <li>HOST:   <ul> <li>description:   The value of the <code>Host</code> HTTP header to send with the API request.   Most frequently constructed as: servicename.region.amazonaws.com</li> <li>example: cloudtrail.us-east-1.amazonaws.com</li> <li>required: yes</li> </ul> </li> <li>REGION: <ul> <li>description:  Region the target infrastructure is located. </li> <li>example: us-east-1</li> <li>required: yes</li> </ul> </li> <li>SERVICE: <ul> <li>description: Name of the service the targeted API belongs to.</li> <li>example: cloudtrail</li> <li>required: yes</li> </ul> </li> <li>ENDPOINT: <ul> <li>description: The full URL of the API call. Commonly \u201chttps:// + host header value\u201d</li> <li>example: https://cloudtrail.us-east-1.amazonaws.com</li> <li>required: yes</li> </ul> </li> <li>VERB: <ul> <li>description: HTTP verb to send the request as.</li> <li>example: POST</li> <li>required: yes</li> </ul> </li> <li>BODY: <ul> <li>description: If POST or PUT request, the Body of the HTTP request to send.</li> <li>example: '{\"Name\": \"derf-trail\"}'</li> <li>required: no</li> </ul> </li> <li>UA: <ul> <li>description: The value of the <code>User-Agent</code> HTTP header to send with the API request.  Using the pattern below, they are recorded as unique per workflow execution, helpful in identifying attack executions within logs. </li> <li>example: '$${\"Derf-AWS-Delete-CloudTrail==\"+sys.get_env(\"GOOGLE_CLOUD_WORKFLOW_EXECUTION_ID\")}'</li> <li>required: no</li> </ul> </li> <li>CONTENT: <ul> <li>description: The value of the <code>Content-Type</code> HTTP header to send with the API request.</li> <li>example: \"application/x-amz-json-1.1\"</li> <li>required: no</li> </ul> </li> <li>USER: <ul> <li>description: The DeRF Execution User (either 01 or 02) to run the attack as    </li> <li>example: $${user}</li> <li>required: no</li> </ul> </li> <li>TARGET: <ul> <li>description: The value of the <code>X-Amz-Target</code> HTTP header to send with the API request. Some AWS APIs require this HTTP header to interface with the API. Proxy and record API calls from your AWS CLI traffic to understand if the API you are working with requires this header.         </li> <li>example: com.amazonaws.cloudtrail.v20131101.CloudTrail_20131101.DeleteTrail</li> <li>required: no</li> </ul> </li> <li>TEMPCREDSPASSED: <ul> <li>description: When \u2018yes\u2019, indicates to the downstream aws proxy application to except credentials sent in the HTTP header and to run the attack as.    </li> <li>example: yes</li> <li>required: no</li> </ul> </li> <li>ACCESSKEYID, ACCESSKEYSECRET &amp; SESSIONTOKEN:<ul> <li>description: When included in the Google Workflow request, these temporary credentials are used by the application to run the attack as.    </li> <li>example:<ul> <li>ACCESSKEYID: '$${ACCESSKEYID}'</li> <li>ACCESSKEYSECRET: '$${ACCESSKEYSECRET}'</li> <li>SESSIONTOKEN: '$${SESSIONTOKEN}'</li> </ul> </li> <li>required: no</li> </ul> </li> </ul>"},{"location":"user-guide/aws-attack-creation/#sample-google-workflow-step","title":"Sample Google Workflow Step","text":"<pre><code>    - DeleteTrail:\ncall: http.post\nargs:\nurl: '$${appEndpoint+\"/submitRequest\"}'\nauth:\ntype: OIDC\nheaders:\nContent-Type: application/json\nbody:\nHOST: cloudtrail.us-east-1.amazonaws.com\nREGION: \"us-east-1\"\nSERVICE: \"cloudtrail\" ENDPOINT: \"https://cloudtrail.us-east-1.amazonaws.com\"\nBODY: '{\"Name\": \"derf-trail\"}'\nUA: '$${\"Derf-AWS-Delete-CloudTrail==\"+sys.get_env(\"GOOGLE_CLOUD_WORKFLOW_EXECUTION_ID\")}'\nCONTENT: \"application/x-amz-json-1.1\"\nUSER: $${user}\nVERB: POST\nTARGET: com.amazonaws.cloudtrail.v20131101.CloudTrail_20131101.DeleteTrail\nresult: response\n</code></pre>"},{"location":"user-guide/execution-user-permissions/","title":"DeRF Execution User Permissions","text":""},{"location":"user-guide/execution-user-permissions/#derf-execution-permissions","title":"DeRF Execution Permissions","text":"<p>The permissions assigned to the <code>derf-execution-users</code> or the <code>DeRF-Default-User</code> are NOT documented here.  Rather, this is a guide as to where you can find those permissions and how to update them.</p>"},{"location":"user-guide/execution-user-permissions/#execution-user-group-membership","title":"Execution User Group Membership","text":"<p>Both User 01 and 02 are members of the <code>derf-execution-users</code> group, allowing them to perform all the same attack techniques outlined in the DeRF.</p>"},{"location":"user-guide/execution-user-permissions/#derf-execution-user-policy-assignments","title":"DeRF Execution User - Policy Assignments","text":"<p>Every attack technique is responsible for creating a policy containing the permissions needed to execute the attack and assigning it to the  <code>derf-execution-users</code> group. The policy and group assignments are found in the <code>iam-permissions.tf</code> file within the <code>/attack-techniques/aws/permissions-required</code> module.   </p>"},{"location":"user-guide/execution-user-permissions/#derf-default-user-policy-assignments","title":"DeRF Default User - Policy Assignments","text":"<p>If an attack technique needs the default user to reverse a state changing action or perform another task that should not be attributed to the execution users, the module is responsible for creating a policy containing the necessary permissions and attaching it to the <code>DeRF-Default-User</code>. The policy and group assignments are found in the <code>iam-permissions.tf</code> file within the <code>/attack-techniques/aws/permissions-required</code> module.    </p>"},{"location":"user-guide/getting-started/","title":"Getting Started","text":""},{"location":"user-guide/getting-started/#step-1-deployment","title":"Step 1 - Deployment","text":"<p>Ensure you have deployed the DeRF resources.  The deployment includes resources in both a targeted AWS Account and the DeRF Framework components housed in a GCP Project.  See the Deployment Guide for detailed instructions.</p>"},{"location":"user-guide/getting-started/#step-2-attack-access-control","title":"Step 2 - Attack Access Control","text":"<p>The ability to execute an attack corresponds to the ability to invoke Cloud Workflows in your DeRF GCP Project. See the Attack Execution Access Control Guide for detailed instructions on how to assign permissions to End Users for attack execution and troubleshooting.</p>"},{"location":"user-guide/getting-started/#step-3-attack-execution","title":"Step 3 - Attack Execution","text":"<p>Attack Techniques are codified as Google Cloud Workflows.  They can be executed as one of two predefined Users and from either the Google Cloud Console or programmatically with the <code>gcloud cli</code>.         - See detailed instructions for executing the attacks from the Google Cloud Console    - See detailed instructions for executing the attacks with the <code>gcloud cli</code></p>"},{"location":"user-guide/programmatic-usage/","title":"Programmatic Usage","text":"<ol> <li>Ensure the Google command line tool is installed on your local system.  Reference Google maintained documentation for instructions on installing <code>gcloud cli</code></li> <li>Authenticate to Google Cloud Project which DeRF is deployed. <pre><code>gcloud auth login --project PROJECT-ID\n</code></pre></li> <li>Invoke a particular attack techniques' workflow with the <code>gcloud cli</code>. See Google documentation for more comprehensive instructions on the workflows service. <pre><code>gcloud workflows run WORKFLOW-NAME `--data={\"user\": \"user01\"}` </code></pre></li> </ol>"},{"location":"user-guide/troubleshooting/","title":"Troubleshooting","text":""},{"location":"user-guide/troubleshooting/#troubleshooting-derf-deployment","title":"Troubleshooting DeRF Deployment","text":"<ol> <li> <p>CLI Error Message:</p> <p>\"You are not authenticated against AWS, or you have not set your region.\"  </p> <ul> <li>You must be authenticated to AWS (and GCP) before deploying the DeRF via Terraform. <pre><code>aws sso login --profile PROFILE_NAME\n</code></pre></li> </ul> </li> <li> <p>CLI Error Message:</p> <p>Error: error configuring S3 Backend: no valid credential sources for S3 Backend found. \u2502 \u2502 Please see https://www.terraform.io/docs/language/settings/backends/s3.html \u2502 for more information about providing credentials. \u2502 \u2502 Error: SSOProviderInvalidToken: the SSO session has expired or is invalid</p> <ul> <li>You must be authenticated to AWS (and GCP) before deploying the DeRF via Terraform.  <pre><code>aws sso login --profile PROFILE-NAME\n</code></pre></li> </ul> </li> <li> <p>CLI Error Message:</p> <p>\"\u2502 Error: Failed to read variables file\"</p> <ul> <li>When running <code>terraform apply -var-file=derf.tfvars</code> the program must be to find the specified variables file. Ensure you are in the <code>./env-prod</code> directory when applying the terraform. Ensure the <code>.tfvars</code> file you specified is in your path.</li> </ul> </li> <li> <p>CLI Error Message:</p> <p>Error: Error creating Trigger: googleapi: Error 400: Repository mapping does not exist. Please visit https://console.cloud.google.com/i connect a repository to your project.</p> <ul> <li>Connect Cloud Build to the <code>derf</code> github repo.  Following instructions in Deployment.</li> </ul> </li> </ol>"},{"location":"user-guide/troubleshooting/#troubleshooting-attack-execution","title":"Troubleshooting Attack Execution","text":"<ol> <li>Error Message on the Google Cloud Console: <p>KeyError: key not found: user</p> </li> </ol> <p></p> <ul> <li>All workflows need to be executed with either User01 or User02. Do so by sending JSON input during workflow execution.  <ul> <li>Input Required:<ul> <li>{\"user\":\"user01\"} OR {\"user\":\"user02\"}</li> </ul> </li> </ul> </li> </ul>"},{"location":"user-guide/usage/","title":"Executing An Attack","text":"<p>Attacks execution is performed by invoking a Google Cloud Workflow. Workflows can be invoked either on the Google Cloud Console or programmatically with the <code>gcloud cli</code></p>"},{"location":"user-guide/usage/#attack-execution-on-the-console","title":"Attack Execution -  on the Console","text":"<ol> <li>Log into the Google Cloud Console and navigate to the workflows page.</li> <li>Click on the name of the workflow that matches the attach you want to execute. </li> <li>Click on the execute button. </li> <li>Refer to the code panel on the right-hand side and select which user to run the attack as by copying one of the possible inputs. </li> <li>Paste selected json in the input panel on the left-hand side. </li> <li>Finally, select the <code>Execute</code> button at the bottom of the screen.  The results of the attack will be displayed on the right-hand side of the screen.</li> </ol>"}]}